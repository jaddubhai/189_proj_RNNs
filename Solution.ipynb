{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## Learning stock embeddings for portfolio optimization using bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Gated Recurrent Units (Bi-GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bi-GRUs for price movement classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, we will focus on training a classifier for 15 stocks from the S&P 500. The goal of our classifier is as follows:\n",
    "We are interested in training a bidirectional RNN model that learns a relationship between news taglines related to the 15 stocks $\\{l_1, \\ldots, l_{15}\\}$ that we have selected and the prices of those stocks. Define $p_i^{(t)}$ to be the price of stock $l_i$ on day $t$. Then, we can formally define our objective as follows:\n",
    "\n",
    "Let $y_i^{(t)} = \\begin{cases} 1 & p_i^{(t)} \\geq p_i^{(t - 1)} \\\\ 0 & p_i^{(t)} < p_i^{(t - 1)} \\end{cases}$. Suppose our dataset $D = \\{N^{(t)}\\}_{t_{in} \\leq t \\leq t_f}$, where $N^{(t)}$ is a collection of all the articles from day $t$ and $t_{in}$ and $t_f$ represent the dates of the earliest and latest articles in our dataset resepctively. Then, we want to learn a mapping $\\hat y_i^{(t)} = f(N^{(t - \\mu)} \\cup \\ldots \\cup N^{(t)})$ such that $\\hat y_i^{(t)}$ accurately predicts $y_i^{(t)}$. More specifically, as is often the case with classification problems, we want to minimize the loss function given by the mean cross-entropy loss for all $15$ stocks:\n",
    "$$\\mathcal{L} = \\frac{1}{15} \\sum_{i = 1}^{15} \\mathcal{L}_i = \\frac{1}{15} \\sum_{i = 1}^{15} \\left( \\frac{-1}{t_f - t_{in}} \\sum_{t = t_{in}}^{t_f} \\big(y_i^{(t)} \\log \\hat y_i^{(t)} + (1 - y_i^{(t)}) \\log (1 - \\hat y_i^{(t)}) \\right)$$\n",
    "Here, we choose to use $\\mu = 4$, so we aim to classify the price movement of stock $l_i$ on day $t$, given by $p_i^{(t)}$, using news information from days $[t-4, t]$, i.e., articles $\\{N^{(t - 4)}, N^{(t - 3)}, N^{(t - 2)}, N^{(t - 1)}, N^{(t)}\\}$. Notice that we are including information from day $t$, so we are not *predicting* the price movement but rather identifying a relationship between the stock price movement and the information contained in the news taglines from day $t$ and the previous 4 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loads word embeddings that we have pre-generated for 15 stocks from the S&P 500. We used news tagline data from Reuters (data sourced from https://github.com/vedic-partap/Event-Driven-Stock-Prediction-using-Deep-Learning/blob/master/input/news_reuters.csv) to create word embeddings for all of the articles in our dataset using a pretrained Spacy encoder and a Word2Vec model that we trained on our data (don't worry if you don't know what this means yet). Our dataset contains news articles from 2011 to 2017 so we should have enough data to build a fairly accurate classifier. You will explore algorithms for generating word embeddings in more detail later in the course but for this assignment, we have done the work for you so that you can focus on building RNN models for your stock movement classifier.\n",
    "\n",
    "For the purposes of our classifier, we are focusing on the 15 stocks from the Reuters dataset for which we have the most data, i.e., news articles.\n",
    "\n",
    "<br>\n",
    "\n",
    "The main idea is to convert all of the qualitative textual information that we have in each article tagline into a quantitative feature that we can use when training our classifier. Let $s_i \\in \\mathbb{R}^{64}$ represent the stock embedding that we are trying to learn for stock $l_i$. We then define the following quantities:\n",
    "\n",
    "Let $n_i^{(t)}$ be a news article from day $t$, for some $1 \\leq i \\leq |N^{(t)}|$. We associate 2 embedding vectors $K_i^{(t)} \\in \\mathbb{R}^{64}$ and $V_i^{(t)} \\in \\mathbb{R}^{300}$ with the article $n_i^{(t)}$, which we have computed for you below. We define $score(n_i^{(t)}, s_j) = K_i^{(t)} \\cdot s_j$ and the softmax variable $$\\alpha_i^{(t)} = \\frac{\\exp(score(n_i^{(t)}, s_j))}{\\sum_{n_k^{(t)} \\in N^{(t)}}exp(score(n_k^{(t)}, s_j))}$$\n",
    "\n",
    "Finally, we define the market status of stock $m_j$ on day $t$, given by $m_j^{(t)} = \\sum_{n_i^{(t)} \\in N^{(t)}} \\alpha_i^{(t)} V_i^{(t)}$. This is the input to the classifier that you will build and train on the dataset to learn the stock embeddings $\\{s_j\\}_{1 \\leq j \\leq 15}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Tagline</th>\n",
       "      <th>Rating</th>\n",
       "      <th>K0</th>\n",
       "      <th>K1</th>\n",
       "      <th>K2</th>\n",
       "      <th>...</th>\n",
       "      <th>V290</th>\n",
       "      <th>V291</th>\n",
       "      <th>V292</th>\n",
       "      <th>V293</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V297</th>\n",
       "      <th>V298</th>\n",
       "      <th>V299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1074</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK Apple Inc has made a \"promising start...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>0.728133</td>\n",
       "      <td>0.074376</td>\n",
       "      <td>-0.844244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184006</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>0.032128</td>\n",
       "      <td>-0.045440</td>\n",
       "      <td>0.027079</td>\n",
       "      <td>-0.100620</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>-0.092093</td>\n",
       "      <td>0.048542</td>\n",
       "      <td>0.109286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1075</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK  April 14 Apple Inc has made a \"promi...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.757790</td>\n",
       "      <td>0.111567</td>\n",
       "      <td>-0.802569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168789</td>\n",
       "      <td>0.039603</td>\n",
       "      <td>0.021292</td>\n",
       "      <td>-0.036883</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>-0.110353</td>\n",
       "      <td>0.025347</td>\n",
       "      <td>-0.084554</td>\n",
       "      <td>0.045670</td>\n",
       "      <td>0.105747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1076</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>COLUMN-How to avoid the trouble coming to the ...</td>\n",
       "      <td>(The opinions expressed here are those of the ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.624152</td>\n",
       "      <td>-0.346050</td>\n",
       "      <td>-1.487509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141506</td>\n",
       "      <td>-0.027039</td>\n",
       "      <td>-0.080825</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>0.018669</td>\n",
       "      <td>-0.056828</td>\n",
       "      <td>-0.052640</td>\n",
       "      <td>-0.169819</td>\n",
       "      <td>-0.033054</td>\n",
       "      <td>0.053817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1077</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>How to avoid the trouble coming to the tech se...</td>\n",
       "      <td>CHICAGO A resounding shot across the bow has b...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.387120</td>\n",
       "      <td>-0.099557</td>\n",
       "      <td>-0.590867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233473</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.113241</td>\n",
       "      <td>-0.027537</td>\n",
       "      <td>-0.119434</td>\n",
       "      <td>-0.074786</td>\n",
       "      <td>-0.072007</td>\n",
       "      <td>-0.049933</td>\n",
       "      <td>0.014863</td>\n",
       "      <td>0.063664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1078</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140415</td>\n",
       "      <td>Apple cannot escape U.S. states' e-book antitr...</td>\n",
       "      <td>NEW YORK Apple Inc on Tuesday lost an attempt ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.824634</td>\n",
       "      <td>-1.637257</td>\n",
       "      <td>-0.352775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232241</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>-0.025965</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>-0.087056</td>\n",
       "      <td>-0.103006</td>\n",
       "      <td>0.076729</td>\n",
       "      <td>-0.153311</td>\n",
       "      <td>0.038894</td>\n",
       "      <td>0.138866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50787</th>\n",
       "      <td>184859</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170209</td>\n",
       "      <td>BRIEF-Ultra Petroleum says Barclays agreed to ...</td>\n",
       "      <td>* Ultra Petroleum- on Feb 8  in connection wit...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.139437</td>\n",
       "      <td>0.682006</td>\n",
       "      <td>0.029171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244216</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>-0.008725</td>\n",
       "      <td>-0.048169</td>\n",
       "      <td>-0.032766</td>\n",
       "      <td>-0.062842</td>\n",
       "      <td>-0.059161</td>\n",
       "      <td>-0.104091</td>\n",
       "      <td>0.010547</td>\n",
       "      <td>0.129130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50788</th>\n",
       "      <td>184860</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170209</td>\n",
       "      <td>MOVES-Barclays  Nasdaq  RenCap  AXA  BC Partners</td>\n",
       "      <td>Feb 9 The following financial services industr...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>1.017802</td>\n",
       "      <td>-0.165982</td>\n",
       "      <td>-0.467275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234947</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>0.064670</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>-0.144732</td>\n",
       "      <td>-0.046366</td>\n",
       "      <td>-0.030195</td>\n",
       "      <td>-0.027131</td>\n",
       "      <td>0.093039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50789</th>\n",
       "      <td>184861</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>Barclays  Citi gave South Africa watchdog info...</td>\n",
       "      <td>JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.044449</td>\n",
       "      <td>-0.042930</td>\n",
       "      <td>0.201579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234416</td>\n",
       "      <td>-0.001098</td>\n",
       "      <td>-0.035648</td>\n",
       "      <td>-0.053637</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>-0.037331</td>\n",
       "      <td>0.048593</td>\n",
       "      <td>-0.019262</td>\n",
       "      <td>-0.030251</td>\n",
       "      <td>0.178724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50790</th>\n",
       "      <td>184862</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>Barclays  Citi helped South Africa with forex ...</td>\n",
       "      <td>JOHANNESBURG Barclays Plc  and Citigroup  appr...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>1.288937</td>\n",
       "      <td>-0.372697</td>\n",
       "      <td>0.197727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247672</td>\n",
       "      <td>0.049712</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>-0.078167</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>0.061589</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>-0.073754</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>0.154577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50791</th>\n",
       "      <td>184863</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>UPDATE 1-Barclays  Citi helped South Africa wi...</td>\n",
       "      <td>JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.336899</td>\n",
       "      <td>-0.299033</td>\n",
       "      <td>0.236128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>-0.072232</td>\n",
       "      <td>0.047836</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>-0.067272</td>\n",
       "      <td>-0.009805</td>\n",
       "      <td>0.183828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50792 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index Ticker                                        Name      Date  \\\n",
       "0        1074   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "1        1075   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "2        1076   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "3        1077   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "4        1078   AAPL                        1-800 FLOWERSCOM Inc  20140415   \n",
       "...       ...    ...                                         ...       ...   \n",
       "50787  184859   TAPR  Barclays Inverse US Treasury Composite ETN  20170209   \n",
       "50788  184860   TAPR  Barclays Inverse US Treasury Composite ETN  20170209   \n",
       "50789  184861   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "50790  184862   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "50791  184863   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "\n",
       "                                                Headline  \\\n",
       "0      Apple antitrust compliance off to a promising ...   \n",
       "1      Apple antitrust compliance off to a promising ...   \n",
       "2      COLUMN-How to avoid the trouble coming to the ...   \n",
       "3      How to avoid the trouble coming to the tech se...   \n",
       "4      Apple cannot escape U.S. states' e-book antitr...   \n",
       "...                                                  ...   \n",
       "50787  BRIEF-Ultra Petroleum says Barclays agreed to ...   \n",
       "50788  MOVES-Barclays  Nasdaq  RenCap  AXA  BC Partners    \n",
       "50789  Barclays  Citi gave South Africa watchdog info...   \n",
       "50790  Barclays  Citi helped South Africa with forex ...   \n",
       "50791  UPDATE 1-Barclays  Citi helped South Africa wi...   \n",
       "\n",
       "                                                 Tagline    Rating        K0  \\\n",
       "0      NEW YORK Apple Inc has made a \"promising start...  topStory  0.728133   \n",
       "1      NEW YORK  April 14 Apple Inc has made a \"promi...    normal  0.757790   \n",
       "2      (The opinions expressed here are those of the ...    normal -0.624152   \n",
       "3      CHICAGO A resounding shot across the bow has b...    normal  0.387120   \n",
       "4      NEW YORK Apple Inc on Tuesday lost an attempt ...    normal  0.824634   \n",
       "...                                                  ...       ...       ...   \n",
       "50787  * Ultra Petroleum- on Feb 8  in connection wit...    normal  1.139437   \n",
       "50788  Feb 9 The following financial services industr...  topStory  1.017802   \n",
       "50789  JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...    normal  1.044449   \n",
       "50790  JOHANNESBURG Barclays Plc  and Citigroup  appr...  topStory  1.288937   \n",
       "50791  JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...    normal  1.336899   \n",
       "\n",
       "             K1        K2  ...      V290      V291      V292      V293  \\\n",
       "0      0.074376 -0.844244  ... -0.184006  0.032116  0.032128 -0.045440   \n",
       "1      0.111567 -0.802569  ... -0.168789  0.039603  0.021292 -0.036883   \n",
       "2     -0.346050 -1.487509  ... -0.141506 -0.027039 -0.080825 -0.133556   \n",
       "3     -0.099557 -0.590867  ... -0.233473  0.095700  0.113241 -0.027537   \n",
       "4     -1.637257 -0.352775  ... -0.232241  0.027836 -0.025965  0.036613   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "50787  0.682006  0.029171  ... -0.244216  0.053853 -0.008725 -0.048169   \n",
       "50788 -0.165982 -0.467275  ... -0.234947  0.049924  0.064670  0.022008   \n",
       "50789 -0.042930  0.201579  ... -0.234416 -0.001098 -0.035648 -0.053637   \n",
       "50790 -0.372697  0.197727  ... -0.247672  0.049712  0.028656 -0.078167   \n",
       "50791 -0.299033  0.236128  ... -0.244397  0.040046  0.010388 -0.072232   \n",
       "\n",
       "           V294      V295      V296      V297      V298      V299  \n",
       "0      0.027079 -0.100620  0.032597 -0.092093  0.048542  0.109286  \n",
       "1      0.029685 -0.110353  0.025347 -0.084554  0.045670  0.105747  \n",
       "2      0.018669 -0.056828 -0.052640 -0.169819 -0.033054  0.053817  \n",
       "3     -0.119434 -0.074786 -0.072007 -0.049933  0.014863  0.063664  \n",
       "4     -0.087056 -0.103006  0.076729 -0.153311  0.038894  0.138866  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "50787 -0.032766 -0.062842 -0.059161 -0.104091  0.010547  0.129130  \n",
       "50788  0.025572 -0.144732 -0.046366 -0.030195 -0.027131  0.093039  \n",
       "50789  0.030076 -0.037331  0.048593 -0.019262 -0.030251  0.178724  \n",
       "50790  0.047243  0.061589  0.016127 -0.073754 -0.011532  0.154577  \n",
       "50791  0.047836  0.046529  0.009858 -0.067272 -0.009805  0.183828  \n",
       "\n",
       "[50792 rows x 371 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"embeddings.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each row represents a different news article and is associated with one of the top 15 stocks that we are interested in for our classifier: <br>\n",
    "`['AAPL', 'AMZN', 'BA', 'BCS', 'BP', 'C', 'DB', 'GM', 'GS', 'HSEA', 'HSEB', 'JPM', 'MSFT', 'MS', 'TAPR']`.\n",
    "\n",
    "Additionally, the columns `[K0, ..., K63]` represent the components of the $K_i^{(t)}$ embedding vector and the columns `[V0, ..., V299]` represent the components of the $V_i^{(t)}$ embedding vector for each article $n_i^{(t)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bi-GRU price movement classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) a) Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6674"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## do it for one stock, AAPL\n",
    "aapl = data[data['Ticker'] == 'AAPL']\n",
    "len(aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set kappa to be max number of articles for a given day\n",
    "kappa = np.max(aapl.groupby('Date').count()['index'])\n",
    "kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove dates that have < 4 articles\n",
    "drop_dates = set(aapl['Date'].unique()[(aapl.groupby('Date').count()['index'] < 4)])\n",
    "drop_indices = [not aapl['Date'][i] in drop_dates for i in range(len(aapl))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now all dates have 4 <= i < 12 articles\n",
    "aapl_processed = aapl[drop_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dates  = sorted(aapl_processed['Date'].unique())\n",
    "num_sequences = len(sorted_dates[4:])\n",
    "num_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad processed df to include kappa entries for each date\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "padded_aapl_proc = aapl_processed.copy(deep = True)\n",
    "\n",
    "for date in aapl_processed['Date'].unique(): \n",
    "    df_date = aapl_processed[aapl_processed['Date'] == date]\n",
    "    \n",
    "    if len(df_date) <  kappa: \n",
    "        row_append = df_date.head(1)\n",
    "        #change value and key vec to be 0 for appended rows\n",
    "        row_append.iloc[:, row_append.columns.get_loc('K0') : row_append.columns.get_loc('V299') + 1] = 0\n",
    "        \n",
    "        #temp variable to fix annpying pandas append tendencies\n",
    "        temp = padded_aapl_proc\n",
    "        for i in range(kappa - len(df_date)): \n",
    "            temp = temp.append(row_append, ignore_index = True)\n",
    "            \n",
    "        padded_aapl_proc = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_aapl_proc = padded_aapl_proc.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_aapl_proc)/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have processed our data to include only robust inputs, let's do a quick refresher of what your initial input to the neural network is supposed to look like, and what dimensions it will have. Our key vectors for day $t$ are  $K_i^{(t)} \\in \\mathbb{R}^{64}$, and we have at most $\\kappa$ articles per day. Thus, for any given day $t$, we can treat the input as $\\begin{bmatrix} K_1^{(t)} & \\cdots & K_\\kappa^{(t)} \\end{bmatrix} \\in \\mathbb{R}^{64 \\times \\kappa}$. Since our network uses five market vectors ($m^{(t - 4)}, \\ldots, m^{(t)}$) for predicting stock price movement on any given day, we must pass in a sequence of $\\kappa \\cdot 5$ key vectors. \n",
    "\n",
    "So each input looks like $\\begin{bmatrix} K_1^{(t - 4)} & \\cdots & K_\\kappa^{(t - 4)} & \\cdots \\cdots & K_1^{(t)} & \\cdots & K_\\kappa^{(t)} \\end{bmatrix} \\in \\mathbb{R}^{64 \\times 5\\kappa}$. Then, assuming we have $k$ such datapoints (or in our case, $5$ day sequences) in our training dataset, our input is thus:\n",
    "$\\begin{bmatrix} \n",
    "K_1^{(t_1 - 4)} & \\cdots & K_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & K_1^{(t_1)} & \\cdots & K_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "K_1^{(t_k - 4)} & \\cdots & K_\\kappa^{(t_k - 4)} & \\cdots \\cdots & K_1^{(t_k)} & \\cdots & K_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{64k \\times 5\\kappa}$\n",
    "where each row represents a different sequence of $5$ days for the stock key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = num_sequences\n",
    "X_in = np.zeros((64*k, 5*kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through rows, step size of 64 to account for size of key vectors\n",
    "for i in range(0, 64*num_sequences, 64): \n",
    "    dates = sorted_dates[i : i + 5]\n",
    "    \n",
    "    #counter to keep track of column index\n",
    "    counter = 0\n",
    "    for date in dates: \n",
    "        df = aapl_processed[aapl_processed['Date'] == date]\n",
    "        sub_mat = np.array(df.iloc[:, df.columns.get_loc('K0') : df.columns.get_loc('K63') + 1]).T\n",
    "        X_in[i : i + 64, counter : counter + sub_mat.shape[1]] = sub_mat\n",
    "        \n",
    "        #increment by kappa to go to next day in sequence\n",
    "        counter += kappa\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50688"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50688, 60)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84720065,  1.99831985,  1.19001225, ...,  1.27729124,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.01052767, -0.82558529, -0.4053802 , ..., -1.03439441,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.99316981, -1.1317625 , -1.35804945, ..., -1.81819254,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) b) Generating classifier labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier aims to predict price movement - in this notebook, we define movement in terms of log returns, the difference in log prices for a given day t, and a preceding day, t-1. Note that due to the nature of the dataset, we do not have a perfectly contiguous sequence of days, however, we use the next best approximation (i.e. t-2, if it is available, in place of t-1). Using these log returns, we binarize price movement: if returns are > 0 on day t, then $ y^t $ = 1, else 0. \n",
    "\n",
    "You can find historical stock price data on finance.yahoo.com, and use close prices to calculate log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv('AAPL.csv')\n",
    "prices['log_returns'] = np.log(prices['Close']) - np.log(prices['Close'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date format\n",
    "prices['Date'] = prices['Date'].apply(lambda x: int(x.replace('-', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>log_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20110131</td>\n",
       "      <td>11.992857</td>\n",
       "      <td>12.144286</td>\n",
       "      <td>11.939285</td>\n",
       "      <td>12.118571</td>\n",
       "      <td>10.369199</td>\n",
       "      <td>377246800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20110201</td>\n",
       "      <td>12.189285</td>\n",
       "      <td>12.344643</td>\n",
       "      <td>12.177857</td>\n",
       "      <td>12.322500</td>\n",
       "      <td>10.543692</td>\n",
       "      <td>426633200</td>\n",
       "      <td>0.016688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20110202</td>\n",
       "      <td>12.301785</td>\n",
       "      <td>12.330358</td>\n",
       "      <td>12.269643</td>\n",
       "      <td>12.297143</td>\n",
       "      <td>10.521996</td>\n",
       "      <td>258955200</td>\n",
       "      <td>-0.002060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20110203</td>\n",
       "      <td>12.278571</td>\n",
       "      <td>12.294286</td>\n",
       "      <td>12.091071</td>\n",
       "      <td>12.265715</td>\n",
       "      <td>10.495105</td>\n",
       "      <td>393797600</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20110204</td>\n",
       "      <td>12.272857</td>\n",
       "      <td>12.382143</td>\n",
       "      <td>12.268214</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>10.588613</td>\n",
       "      <td>321840400</td>\n",
       "      <td>0.008870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date       Open       High        Low      Close  Adj Close     Volume  \\\n",
       "0  20110131  11.992857  12.144286  11.939285  12.118571  10.369199  377246800   \n",
       "1  20110201  12.189285  12.344643  12.177857  12.322500  10.543692  426633200   \n",
       "2  20110202  12.301785  12.330358  12.269643  12.297143  10.521996  258955200   \n",
       "3  20110203  12.278571  12.294286  12.091071  12.265715  10.495105  393797600   \n",
       "4  20110204  12.272857  12.382143  12.268214  12.375000  10.588613  321840400   \n",
       "\n",
       "   log_returns  \n",
       "0          NaN  \n",
       "1     0.016688  \n",
       "2    -0.002060  \n",
       "3    -0.002559  \n",
       "4     0.008870  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate labels for each of the k sequences for the AAPL stock\n",
    "labels = []\n",
    "for date in sorted_dates[4:]:\n",
    "    log_ret = prices[prices['Date'] == date]['log_returns'].values\n",
    "    \n",
    "    if (len(log_ret) == 0): \n",
    "        labels.append(np.random.choice([1, 0]))\n",
    "        continue\n",
    "    \n",
    "    if (log_ret[0]) > 0: \n",
    "        labels.append(1)\n",
    "    else: \n",
    "        labels.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Building the pre-classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our main classifier, recall that we need to do some preprocessing to get from our input above to the market vectors that are being used in the classifier. The documentation for tensorflow and Keras will help a lot with some of the manipulations required for this section. In particular, the documentation for `Lambda` layers, `Dense` layers, `Concatenate` layers, `keras.activations.softmax` (which we have imported for you), and `tf.split` may be helpful. The outline of the necessary preprocessing steps is as follows:\n",
    "\n",
    "1. As above, the input layer is of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "K_1^{(t_1 - 4)} & \\cdots & K_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & K_1^{(t_1)} & \\cdots & K_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "K_1^{(t_k - 4)} & \\cdots & K_\\kappa^{(t_k - 4)} & \\cdots \\cdots & K_1^{(t_k)} & \\cdots & K_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{64k \\times 5\\kappa}$\n",
    "<br>\n",
    "\n",
    "2. By treating the stock embedding $s$ as a weight from the input layer to the first hidden layer, generate layer 1 of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "score_1^{(t_1 - 4)} & \\cdots & score_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & score_1^{(t_1)} & \\cdots & score_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "score_1^{(t_k - 4)} & \\cdots & score_\\kappa^{(t_k - 4)} & \\cdots \\cdots & score_1^{(t_k)} & \\cdots & score_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k \\times 5\\kappa}$ <br>\n",
    "*Hint:* Remember, we define $score_i^{(t)} = K_i^{(t)} \\cdot s$. Moreover, look at the documentation for applying Dense layers to rank $> 2$ tensors in Keras and see if you need to modify the input in some way before the input layer.\n",
    "<br>\n",
    "\n",
    "3. Apply softmax activations appropriately to generate layer 2 of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "\\alpha_1^{(t_1 - 4)} & \\cdots & \\alpha_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & \\alpha_1^{(t_1)} & \\cdots & \\alpha_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "\\alpha_1^{(t_k - 4)} & \\cdots & \\alpha_\\kappa^{(t_k - 4)} & \\cdots \\cdots & \\alpha_1^{(t_k)} & \\cdots & \\alpha_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k \\times 5\\kappa}$ <br>\n",
    "*Hint:* Remember, we define $\\alpha_i^{(t)} = \\displaystyle \\frac{\\exp(score_i^{(t)})}{\\sum_{j \\in [\\kappa], j \\neq i}exp(score_j^{(t)})}$.\n",
    "<br>\n",
    "\n",
    "4. Finally, the last layer before the classifier, layer 3, contains the market vectors and must be of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "m^{(t_1 - 4)} & \\cdots & m^{(t_1)}\\\\ \n",
    "\\vdots & & \\vdots \\\\ \n",
    "m^{(t_k)} & \\cdots & m^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{300k \\times 5}$ <br>\n",
    "*Hint:* Remember, we define $m_i^{(t)} = \\displaystyle \\sum_{i \\in [\\kappa]} \\alpha_i^{(t)} V_i^{(t)}$. You may find the custom activation function that we have defined for you helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2] + [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def market_activation(v):\n",
    "    '''\n",
    "    Input: Matrix v of shape (300k, kappa) whose columns represent value vectors\n",
    "    Returns: Function custom_sum(x)\n",
    "    '''\n",
    "    def custom_sum(x):\n",
    "        '''\n",
    "        Input: Tensor x of shape (None, k, kappa) with alpha values from layer 3\n",
    "        Returns: Tensor m of shape (None, 300k, 1) that represents the market vectors as defined above\n",
    "        '''\n",
    "        k = x.shape[1]\n",
    "        kappa = x.shape[2]\n",
    "        cols = []\n",
    "        for i in range(k):\n",
    "            cols.append(sum([x[:, i, j] * v[:, j] for j in range(kappa)]))\n",
    "        m = np.vstack(cols)\n",
    "        return m\n",
    "    \n",
    "    return custom_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Input, Dense, GRU, Bidirectional, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform input by transposing each key vector in the above matrix, i.e. 64k x 5kappa -> k x (64 * 5kappa)\n",
    "rows = []\n",
    "for i in range(0, 64 * k, 64):\n",
    "    cols = [X_in[i : i + 64, j] for j in range(5 * kappa)]\n",
    "    to_row = [c.T for c in cols]\n",
    "    rows.append(np.array(to_row).reshape(1, 64 * 5 * kappa))\n",
    "X_in_mod = np.vstack(rows)\n",
    "\n",
    "x = Input(shape = X_in_mod.shape, name=\"Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(x, num_or_size_splits = 5 * kappa, axis=2)\n",
    "#each tensor in cols has shape (None, k, 64)\n",
    "\n",
    "#create (5 * kappa) dense layers, 1 for each tensor in cols, and concatenate them together to get layer 1\n",
    "dense_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    dense_layers.append(Dense(1, use_bias=False)(lambd_layer))\n",
    "\n",
    "layer1 = Concatenate(name=\"Layer1\")(dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(layer1, num_or_size_splits = 5, axis=2)\n",
    "#each tensor in cols has shape (None, k, kappa)\n",
    "\n",
    "#create 5 softmax activations, 1 for each tensor in cols, and concatenate them together to get layer 2\n",
    "softmax_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    softmax_layers.append(softmax(lambd_layer, axis=2))\n",
    "\n",
    "layer2 = Concatenate(name=\"Layer2\")(softmax_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(layer1, num_or_size_splits = 5, axis=2)\n",
    "#each tensor in cols has shape (None, k, kappa)\n",
    "\n",
    "#create 5 market activations, 1 for each tensor in cols, and concatenate them together to get layer 3\n",
    "\n",
    "#counter to keep track of start index for dates\n",
    "counter = 0\n",
    "market_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    \n",
    "    dates = sorted_dates[counter:792 + counter]\n",
    "    \n",
    "    v_inp = np.array([])\n",
    "    \n",
    "    for date in dates: \n",
    "        df = padded_aapl_proc[padded_aapl_proc['Date'] == date]\n",
    "        value_vecs = np.array(padded_aapl_proc.iloc[:, df.columns.get_loc('V0') : df.columns.get_loc('V299') + 1]).T\n",
    "        \n",
    "        if len(v_inp) == 0: \n",
    "            v_inp = value_vecs\n",
    "    \n",
    "        else: \n",
    "            v_inp = np.vstack((v_inp, value_vecs))\n",
    "            \n",
    "    counter += 1\n",
    "    market_layers.append(market_activation(v_inp)(t))\n",
    "\n",
    "layer3 = Concatenate(name=\"Layer3\")(market_layers)\n",
    "\n",
    "'''\n",
    "for i in range(num_sequences*5): \n",
    "    df = aapl_processed.loc[i:i+kappa, :]\n",
    "    value_vecs = np.array(df.iloc[:, df.columns.get_loc('V0') : df.columns.get_loc('V299') + 1]).T\n",
    "    value_vecs = value_vecs.reshape(value_vecs.shape[0] * value_vecs.shape[1], 1)\n",
    "    print(value_vecs.shape)\n",
    "    \n",
    "    alphas_arr = alphas[i]\n",
    "    \n",
    "    inp = tf.concat(alphas_arr, value_vecs) \n",
    "    print(inp.shape)\n",
    "    #need to check how to pass multiple inputs into Dense layer\n",
    "    linear_sums.append(Activation(custom_sum)(inp))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 792, 3840)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_4 (TensorFlow [(None, 792, 64), (N 0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_240 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_241 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_242 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_243 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_244 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_245 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][5]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_246 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][6]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_247 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][7]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_248 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][8]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_249 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][9]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_250 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][10]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_251 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][11]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_252 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][12]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_253 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][13]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_254 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][14]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_255 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][15]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_256 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][16]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_257 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][17]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_258 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][18]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_259 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][19]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_260 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][20]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_261 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][21]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_262 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][22]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_263 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][23]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_264 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][24]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_265 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][25]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_266 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][26]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_267 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][27]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_268 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][28]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_269 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][29]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_270 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][30]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_271 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][31]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_272 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][32]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_273 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][33]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_274 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][34]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_275 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][35]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_276 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][36]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_277 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][37]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_278 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][38]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_279 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][39]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_280 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][40]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_281 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][41]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_282 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][42]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_283 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][43]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_284 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][44]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_285 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][45]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_286 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][46]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_287 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][47]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_288 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][48]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_289 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][49]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_290 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][50]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_291 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][51]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_292 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][52]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_293 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][53]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_294 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][54]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_295 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][55]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_296 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][56]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_297 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][57]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_298 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][58]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_299 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][59]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_240 (Dense)               (None, 792, 1)       64          lambda_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_241 (Dense)               (None, 792, 1)       64          lambda_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_242 (Dense)               (None, 792, 1)       64          lambda_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_243 (Dense)               (None, 792, 1)       64          lambda_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_244 (Dense)               (None, 792, 1)       64          lambda_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_245 (Dense)               (None, 792, 1)       64          lambda_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_246 (Dense)               (None, 792, 1)       64          lambda_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_247 (Dense)               (None, 792, 1)       64          lambda_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_248 (Dense)               (None, 792, 1)       64          lambda_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_249 (Dense)               (None, 792, 1)       64          lambda_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_250 (Dense)               (None, 792, 1)       64          lambda_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_251 (Dense)               (None, 792, 1)       64          lambda_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_252 (Dense)               (None, 792, 1)       64          lambda_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_253 (Dense)               (None, 792, 1)       64          lambda_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_254 (Dense)               (None, 792, 1)       64          lambda_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_255 (Dense)               (None, 792, 1)       64          lambda_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_256 (Dense)               (None, 792, 1)       64          lambda_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_257 (Dense)               (None, 792, 1)       64          lambda_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_258 (Dense)               (None, 792, 1)       64          lambda_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_259 (Dense)               (None, 792, 1)       64          lambda_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_260 (Dense)               (None, 792, 1)       64          lambda_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_261 (Dense)               (None, 792, 1)       64          lambda_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_262 (Dense)               (None, 792, 1)       64          lambda_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_263 (Dense)               (None, 792, 1)       64          lambda_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_264 (Dense)               (None, 792, 1)       64          lambda_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_265 (Dense)               (None, 792, 1)       64          lambda_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_266 (Dense)               (None, 792, 1)       64          lambda_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_267 (Dense)               (None, 792, 1)       64          lambda_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_268 (Dense)               (None, 792, 1)       64          lambda_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_269 (Dense)               (None, 792, 1)       64          lambda_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_270 (Dense)               (None, 792, 1)       64          lambda_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_271 (Dense)               (None, 792, 1)       64          lambda_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_272 (Dense)               (None, 792, 1)       64          lambda_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_273 (Dense)               (None, 792, 1)       64          lambda_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_274 (Dense)               (None, 792, 1)       64          lambda_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_275 (Dense)               (None, 792, 1)       64          lambda_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_276 (Dense)               (None, 792, 1)       64          lambda_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_277 (Dense)               (None, 792, 1)       64          lambda_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_278 (Dense)               (None, 792, 1)       64          lambda_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_279 (Dense)               (None, 792, 1)       64          lambda_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_280 (Dense)               (None, 792, 1)       64          lambda_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_281 (Dense)               (None, 792, 1)       64          lambda_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_282 (Dense)               (None, 792, 1)       64          lambda_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_283 (Dense)               (None, 792, 1)       64          lambda_283[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_284 (Dense)               (None, 792, 1)       64          lambda_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_285 (Dense)               (None, 792, 1)       64          lambda_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_286 (Dense)               (None, 792, 1)       64          lambda_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_287 (Dense)               (None, 792, 1)       64          lambda_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_288 (Dense)               (None, 792, 1)       64          lambda_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_289 (Dense)               (None, 792, 1)       64          lambda_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_290 (Dense)               (None, 792, 1)       64          lambda_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_291 (Dense)               (None, 792, 1)       64          lambda_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_292 (Dense)               (None, 792, 1)       64          lambda_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_293 (Dense)               (None, 792, 1)       64          lambda_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_294 (Dense)               (None, 792, 1)       64          lambda_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_295 (Dense)               (None, 792, 1)       64          lambda_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_296 (Dense)               (None, 792, 1)       64          lambda_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_297 (Dense)               (None, 792, 1)       64          lambda_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_298 (Dense)               (None, 792, 1)       64          lambda_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_299 (Dense)               (None, 792, 1)       64          lambda_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Layer1 (Concatenate)            (None, 792, 60)      0           dense_240[0][0]                  \n",
      "                                                                 dense_241[0][0]                  \n",
      "                                                                 dense_242[0][0]                  \n",
      "                                                                 dense_243[0][0]                  \n",
      "                                                                 dense_244[0][0]                  \n",
      "                                                                 dense_245[0][0]                  \n",
      "                                                                 dense_246[0][0]                  \n",
      "                                                                 dense_247[0][0]                  \n",
      "                                                                 dense_248[0][0]                  \n",
      "                                                                 dense_249[0][0]                  \n",
      "                                                                 dense_250[0][0]                  \n",
      "                                                                 dense_251[0][0]                  \n",
      "                                                                 dense_252[0][0]                  \n",
      "                                                                 dense_253[0][0]                  \n",
      "                                                                 dense_254[0][0]                  \n",
      "                                                                 dense_255[0][0]                  \n",
      "                                                                 dense_256[0][0]                  \n",
      "                                                                 dense_257[0][0]                  \n",
      "                                                                 dense_258[0][0]                  \n",
      "                                                                 dense_259[0][0]                  \n",
      "                                                                 dense_260[0][0]                  \n",
      "                                                                 dense_261[0][0]                  \n",
      "                                                                 dense_262[0][0]                  \n",
      "                                                                 dense_263[0][0]                  \n",
      "                                                                 dense_264[0][0]                  \n",
      "                                                                 dense_265[0][0]                  \n",
      "                                                                 dense_266[0][0]                  \n",
      "                                                                 dense_267[0][0]                  \n",
      "                                                                 dense_268[0][0]                  \n",
      "                                                                 dense_269[0][0]                  \n",
      "                                                                 dense_270[0][0]                  \n",
      "                                                                 dense_271[0][0]                  \n",
      "                                                                 dense_272[0][0]                  \n",
      "                                                                 dense_273[0][0]                  \n",
      "                                                                 dense_274[0][0]                  \n",
      "                                                                 dense_275[0][0]                  \n",
      "                                                                 dense_276[0][0]                  \n",
      "                                                                 dense_277[0][0]                  \n",
      "                                                                 dense_278[0][0]                  \n",
      "                                                                 dense_279[0][0]                  \n",
      "                                                                 dense_280[0][0]                  \n",
      "                                                                 dense_281[0][0]                  \n",
      "                                                                 dense_282[0][0]                  \n",
      "                                                                 dense_283[0][0]                  \n",
      "                                                                 dense_284[0][0]                  \n",
      "                                                                 dense_285[0][0]                  \n",
      "                                                                 dense_286[0][0]                  \n",
      "                                                                 dense_287[0][0]                  \n",
      "                                                                 dense_288[0][0]                  \n",
      "                                                                 dense_289[0][0]                  \n",
      "                                                                 dense_290[0][0]                  \n",
      "                                                                 dense_291[0][0]                  \n",
      "                                                                 dense_292[0][0]                  \n",
      "                                                                 dense_293[0][0]                  \n",
      "                                                                 dense_294[0][0]                  \n",
      "                                                                 dense_295[0][0]                  \n",
      "                                                                 dense_296[0][0]                  \n",
      "                                                                 dense_297[0][0]                  \n",
      "                                                                 dense_298[0][0]                  \n",
      "                                                                 dense_299[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_9 (TensorFlow [(None, 792, 12), (N 0           Layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_304 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_305 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_306 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_307 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_308 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max (TensorFlowOpLa [(None, 792, 1)]     0           lambda_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_1 (TensorFlowOp [(None, 792, 1)]     0           lambda_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_2 (TensorFlowOp [(None, 792, 1)]     0           lambda_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_3 (TensorFlowOp [(None, 792, 1)]     0           lambda_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_4 (TensorFlowOp [(None, 792, 1)]     0           lambda_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 792, 12)]    0           lambda_304[0][0]                 \n",
      "                                                                 tf_op_layer_Max[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(None, 792, 12)]    0           lambda_305[0][0]                 \n",
      "                                                                 tf_op_layer_Max_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_2 (TensorFlowOp [(None, 792, 12)]    0           lambda_306[0][0]                 \n",
      "                                                                 tf_op_layer_Max_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_3 (TensorFlowOp [(None, 792, 12)]    0           lambda_307[0][0]                 \n",
      "                                                                 tf_op_layer_Max_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_4 (TensorFlowOp [(None, 792, 12)]    0           lambda_308[0][0]                 \n",
      "                                                                 tf_op_layer_Max_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp (TensorFlowOpLa [(None, 792, 12)]    0           tf_op_layer_Sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_1 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_2 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_3 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_4 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 792, 1)]     0           tf_op_layer_Exp[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_3 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorFlow [(None, 792, 12)]    0           tf_op_layer_Exp[0][0]            \n",
      "                                                                 tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_1 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_1[0][0]          \n",
      "                                                                 tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_2 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_2[0][0]          \n",
      "                                                                 tf_op_layer_Sum_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_3 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_3[0][0]          \n",
      "                                                                 tf_op_layer_Sum_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_4 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_4[0][0]          \n",
      "                                                                 tf_op_layer_Sum_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Layer2 (Concatenate)            (None, 792, 60)      0           tf_op_layer_RealDiv[0][0]        \n",
      "                                                                 tf_op_layer_RealDiv_1[0][0]      \n",
      "                                                                 tf_op_layer_RealDiv_2[0][0]      \n",
      "                                                                 tf_op_layer_RealDiv_3[0][0]      \n",
      "                                                                 tf_op_layer_RealDiv_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,840\n",
      "Trainable params: 3,840\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = x, outputs = layer2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_alphas = Concatenate([linear_sums[i] for i in range(len(linear_sums))])\n",
    "bigru = Bidirectional(GRU(num_sequences, activation = 'relu'))(flattened_alphas)\n",
    "pred = Dense(num_sequences, activation = 'sigmoid')(bigru)\n",
    "\n",
    "model = Model(inputs = x, outputs = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
