{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## Learning stock embeddings for portfolio optimization using bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Gated Recurrent Units (Bi-GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bi-GRUs for price movement classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, we will focus on training a classifier for 15 stocks from the S&P 500. The goal of our classifier is as follows:\n",
    "We are interested in training a bidirectional RNN model that learns a relationship between news taglines related to the 15 stocks $\\{l_1, \\ldots, l_{15}\\}$ that we have selected and the prices of those stocks. Define $p_i^{(t)}$ to be the price of stock $l_i$ on day $t$. Then, we can formally define our objective as follows:\n",
    "\n",
    "Let $y_i^{(t)} = \\begin{cases} 1 & p_i^{(t)} \\geq p_i^{(t - 1)} \\\\ 0 & p_i^{(t)} < p_i^{(t - 1)} \\end{cases}$. Suppose our dataset $D = \\{N^{(t)}\\}_{t_{in} \\leq t \\leq t_f}$, where $N^{(t)}$ is a collection of all the articles from day $t$ and $t_{in}$ and $t_f$ represent the dates of the earliest and latest articles in our dataset resepctively. Then, we want to learn a mapping $\\hat y_i^{(t)} = f(N^{(t - \\mu)} \\cup \\ldots \\cup N^{(t)})$ such that $\\hat y_i^{(t)}$ accurately predicts $y_i^{(t)}$. More specifically, as is often the case with classification problems, we want to minimize the loss function given by the mean cross-entropy loss for all $15$ stocks:\n",
    "$$\\mathcal{L} = \\frac{1}{15} \\sum_{i = 1}^{15} \\mathcal{L}_i = \\frac{1}{15} \\sum_{i = 1}^{15} \\left( \\frac{-1}{t_f - t_{in}} \\sum_{t = t_{in}}^{t_f} \\big(y_i^{(t)} \\log \\hat y_i^{(t)} + (1 - y_i^{(t)}) \\log (1 - \\hat y_i^{(t)}) \\right)$$\n",
    "Here, we choose to use $\\mu = 4$, so we aim to classify the price movement of stock $l_i$ on day $t$, given by $p_i^{(t)}$, using news information from days $[t-4, t]$, i.e., articles $\\{N^{(t - 4)}, N^{(t - 3)}, N^{(t - 2)}, N^{(t - 1)}, N^{(t)}\\}$. Notice that we are including information from day $t$, so we are not *predicting* the price movement but rather identifying a relationship between the stock price movement and the information contained in the news taglines from day $t$ and the previous 4 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loads word embeddings that we have pre-generated for 15 stocks from the S&P 500. We used news tagline data from Reuters (data sourced from https://github.com/vedic-partap/Event-Driven-Stock-Prediction-using-Deep-Learning/blob/master/input/news_reuters.csv) to create word embeddings for all of the articles in our dataset using a pretrained Spacy encoder and a Word2Vec model that we trained on our data (don't worry if you don't know what this means yet). Our dataset contains news articles from 2011 to 2017 so we should have enough data to build a fairly accurate classifier. You will explore algorithms for generating word embeddings in more detail later in the course but for this assignment, we have done the work for you so that you can focus on building RNN models for your stock movement classifier.\n",
    "\n",
    "For the purposes of our classifier, we are focusing on the 15 stocks from the Reuters dataset for which we have the most data, i.e., news articles.\n",
    "\n",
    "<br>\n",
    "\n",
    "The main idea is to convert all of the qualitative textual information that we have in each article tagline into a quantitative feature that we can use when training our classifier. Let $s_i \\in \\mathbb{R}^{64}$ represent the stock embedding that we are trying to learn for stock $l_i$. We then define the following quantities:\n",
    "\n",
    "Let $n_i^{(t)}$ be a news article from day $t$, for some $1 \\leq i \\leq |N^{(t)}|$. We associate 2 embedding vectors $K_i^{(t)} \\in \\mathbb{R}^{64}$ and $V_i^{(t)} \\in \\mathbb{R}^{300}$ with the article $n_i^{(t)}$, which we have computed for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Tagline</th>\n",
       "      <th>Rating</th>\n",
       "      <th>K0</th>\n",
       "      <th>K1</th>\n",
       "      <th>K2</th>\n",
       "      <th>...</th>\n",
       "      <th>V290</th>\n",
       "      <th>V291</th>\n",
       "      <th>V292</th>\n",
       "      <th>V293</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V297</th>\n",
       "      <th>V298</th>\n",
       "      <th>V299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1074</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK Apple Inc has made a \"promising start...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>0.728133</td>\n",
       "      <td>0.074376</td>\n",
       "      <td>-0.844244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184006</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>0.032128</td>\n",
       "      <td>-0.045440</td>\n",
       "      <td>0.027079</td>\n",
       "      <td>-0.100620</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>-0.092093</td>\n",
       "      <td>0.048542</td>\n",
       "      <td>0.109286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1075</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK  April 14 Apple Inc has made a \"promi...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.757790</td>\n",
       "      <td>0.111567</td>\n",
       "      <td>-0.802569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168789</td>\n",
       "      <td>0.039603</td>\n",
       "      <td>0.021292</td>\n",
       "      <td>-0.036883</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>-0.110353</td>\n",
       "      <td>0.025347</td>\n",
       "      <td>-0.084554</td>\n",
       "      <td>0.045670</td>\n",
       "      <td>0.105747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1076</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>COLUMN-How to avoid the trouble coming to the ...</td>\n",
       "      <td>(The opinions expressed here are those of the ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.624152</td>\n",
       "      <td>-0.346050</td>\n",
       "      <td>-1.487509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141506</td>\n",
       "      <td>-0.027039</td>\n",
       "      <td>-0.080825</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>0.018669</td>\n",
       "      <td>-0.056828</td>\n",
       "      <td>-0.052640</td>\n",
       "      <td>-0.169819</td>\n",
       "      <td>-0.033054</td>\n",
       "      <td>0.053817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1077</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>How to avoid the trouble coming to the tech se...</td>\n",
       "      <td>CHICAGO A resounding shot across the bow has b...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.387120</td>\n",
       "      <td>-0.099557</td>\n",
       "      <td>-0.590867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233473</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.113241</td>\n",
       "      <td>-0.027537</td>\n",
       "      <td>-0.119434</td>\n",
       "      <td>-0.074786</td>\n",
       "      <td>-0.072007</td>\n",
       "      <td>-0.049933</td>\n",
       "      <td>0.014863</td>\n",
       "      <td>0.063664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1078</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140415</td>\n",
       "      <td>Apple cannot escape U.S. states' e-book antitr...</td>\n",
       "      <td>NEW YORK Apple Inc on Tuesday lost an attempt ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.824634</td>\n",
       "      <td>-1.637257</td>\n",
       "      <td>-0.352775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232241</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>-0.025965</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>-0.087056</td>\n",
       "      <td>-0.103006</td>\n",
       "      <td>0.076729</td>\n",
       "      <td>-0.153311</td>\n",
       "      <td>0.038894</td>\n",
       "      <td>0.138866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50787</th>\n",
       "      <td>184859</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170209</td>\n",
       "      <td>BRIEF-Ultra Petroleum says Barclays agreed to ...</td>\n",
       "      <td>* Ultra Petroleum- on Feb 8  in connection wit...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.139437</td>\n",
       "      <td>0.682006</td>\n",
       "      <td>0.029171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244216</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>-0.008725</td>\n",
       "      <td>-0.048169</td>\n",
       "      <td>-0.032766</td>\n",
       "      <td>-0.062842</td>\n",
       "      <td>-0.059161</td>\n",
       "      <td>-0.104091</td>\n",
       "      <td>0.010547</td>\n",
       "      <td>0.129130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50788</th>\n",
       "      <td>184860</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170209</td>\n",
       "      <td>MOVES-Barclays  Nasdaq  RenCap  AXA  BC Partners</td>\n",
       "      <td>Feb 9 The following financial services industr...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>1.017802</td>\n",
       "      <td>-0.165982</td>\n",
       "      <td>-0.467275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234947</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>0.064670</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>-0.144732</td>\n",
       "      <td>-0.046366</td>\n",
       "      <td>-0.030195</td>\n",
       "      <td>-0.027131</td>\n",
       "      <td>0.093039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50789</th>\n",
       "      <td>184861</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>Barclays  Citi gave South Africa watchdog info...</td>\n",
       "      <td>JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.044449</td>\n",
       "      <td>-0.042930</td>\n",
       "      <td>0.201579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234416</td>\n",
       "      <td>-0.001098</td>\n",
       "      <td>-0.035648</td>\n",
       "      <td>-0.053637</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>-0.037331</td>\n",
       "      <td>0.048593</td>\n",
       "      <td>-0.019262</td>\n",
       "      <td>-0.030251</td>\n",
       "      <td>0.178724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50790</th>\n",
       "      <td>184862</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>Barclays  Citi helped South Africa with forex ...</td>\n",
       "      <td>JOHANNESBURG Barclays Plc  and Citigroup  appr...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>1.288937</td>\n",
       "      <td>-0.372697</td>\n",
       "      <td>0.197727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247672</td>\n",
       "      <td>0.049712</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>-0.078167</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>0.061589</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>-0.073754</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>0.154577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50791</th>\n",
       "      <td>184863</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>UPDATE 1-Barclays  Citi helped South Africa wi...</td>\n",
       "      <td>JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.336899</td>\n",
       "      <td>-0.299033</td>\n",
       "      <td>0.236128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>-0.072232</td>\n",
       "      <td>0.047836</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>-0.067272</td>\n",
       "      <td>-0.009805</td>\n",
       "      <td>0.183828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50792 rows Ã— 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index Ticker                                        Name      Date  \\\n",
       "0        1074   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "1        1075   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "2        1076   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "3        1077   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "4        1078   AAPL                        1-800 FLOWERSCOM Inc  20140415   \n",
       "...       ...    ...                                         ...       ...   \n",
       "50787  184859   TAPR  Barclays Inverse US Treasury Composite ETN  20170209   \n",
       "50788  184860   TAPR  Barclays Inverse US Treasury Composite ETN  20170209   \n",
       "50789  184861   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "50790  184862   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "50791  184863   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "\n",
       "                                                Headline  \\\n",
       "0      Apple antitrust compliance off to a promising ...   \n",
       "1      Apple antitrust compliance off to a promising ...   \n",
       "2      COLUMN-How to avoid the trouble coming to the ...   \n",
       "3      How to avoid the trouble coming to the tech se...   \n",
       "4      Apple cannot escape U.S. states' e-book antitr...   \n",
       "...                                                  ...   \n",
       "50787  BRIEF-Ultra Petroleum says Barclays agreed to ...   \n",
       "50788  MOVES-Barclays  Nasdaq  RenCap  AXA  BC Partners    \n",
       "50789  Barclays  Citi gave South Africa watchdog info...   \n",
       "50790  Barclays  Citi helped South Africa with forex ...   \n",
       "50791  UPDATE 1-Barclays  Citi helped South Africa wi...   \n",
       "\n",
       "                                                 Tagline    Rating        K0  \\\n",
       "0      NEW YORK Apple Inc has made a \"promising start...  topStory  0.728133   \n",
       "1      NEW YORK  April 14 Apple Inc has made a \"promi...    normal  0.757790   \n",
       "2      (The opinions expressed here are those of the ...    normal -0.624152   \n",
       "3      CHICAGO A resounding shot across the bow has b...    normal  0.387120   \n",
       "4      NEW YORK Apple Inc on Tuesday lost an attempt ...    normal  0.824634   \n",
       "...                                                  ...       ...       ...   \n",
       "50787  * Ultra Petroleum- on Feb 8  in connection wit...    normal  1.139437   \n",
       "50788  Feb 9 The following financial services industr...  topStory  1.017802   \n",
       "50789  JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...    normal  1.044449   \n",
       "50790  JOHANNESBURG Barclays Plc  and Citigroup  appr...  topStory  1.288937   \n",
       "50791  JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...    normal  1.336899   \n",
       "\n",
       "             K1        K2  ...      V290      V291      V292      V293  \\\n",
       "0      0.074376 -0.844244  ... -0.184006  0.032116  0.032128 -0.045440   \n",
       "1      0.111567 -0.802569  ... -0.168789  0.039603  0.021292 -0.036883   \n",
       "2     -0.346050 -1.487509  ... -0.141506 -0.027039 -0.080825 -0.133556   \n",
       "3     -0.099557 -0.590867  ... -0.233473  0.095700  0.113241 -0.027537   \n",
       "4     -1.637257 -0.352775  ... -0.232241  0.027836 -0.025965  0.036613   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "50787  0.682006  0.029171  ... -0.244216  0.053853 -0.008725 -0.048169   \n",
       "50788 -0.165982 -0.467275  ... -0.234947  0.049924  0.064670  0.022008   \n",
       "50789 -0.042930  0.201579  ... -0.234416 -0.001098 -0.035648 -0.053637   \n",
       "50790 -0.372697  0.197727  ... -0.247672  0.049712  0.028656 -0.078167   \n",
       "50791 -0.299033  0.236128  ... -0.244397  0.040046  0.010388 -0.072232   \n",
       "\n",
       "           V294      V295      V296      V297      V298      V299  \n",
       "0      0.027079 -0.100620  0.032597 -0.092093  0.048542  0.109286  \n",
       "1      0.029685 -0.110353  0.025347 -0.084554  0.045670  0.105747  \n",
       "2      0.018669 -0.056828 -0.052640 -0.169819 -0.033054  0.053817  \n",
       "3     -0.119434 -0.074786 -0.072007 -0.049933  0.014863  0.063664  \n",
       "4     -0.087056 -0.103006  0.076729 -0.153311  0.038894  0.138866  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "50787 -0.032766 -0.062842 -0.059161 -0.104091  0.010547  0.129130  \n",
       "50788  0.025572 -0.144732 -0.046366 -0.030195 -0.027131  0.093039  \n",
       "50789  0.030076 -0.037331  0.048593 -0.019262 -0.030251  0.178724  \n",
       "50790  0.047243  0.061589  0.016127 -0.073754 -0.011532  0.154577  \n",
       "50791  0.047836  0.046529  0.009858 -0.067272 -0.009805  0.183828  \n",
       "\n",
       "[50792 rows x 371 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"embeddings.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each row represents a different news article and is associated with one of the top 15 stocks that we are interested in for our classifier: <br>\n",
    "`['AAPL', 'AMZN', 'BA', 'BCS', 'BP', 'C', 'DB', 'GM', 'GS', 'HSEA', 'HSEB', 'JPM', 'MSFT', 'MS', 'TAPR']`.\n",
    "\n",
    "Additionally, the columns `[K0, ..., K63]` represent the components of the $K_i^{(t)}$ embedding vector and the columns `[V0, ..., V299]` represent the components of the $V_i^{(t)}$ embedding vector for each article $n_i^{(t)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bi-GRU price movement classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define $score(n_i^{(t)}, s_j) = K_i^{(t)} \\cdot s_j$ and the softmax variable $$\\alpha_i^{(t)} = \\frac{\\exp(score(n_i^{(t)}, s_j))}{\\sum_{n_k^{(t)} \\in N^{(t)}}exp(score(n_k^{(t)}, s_j))}$$\n",
    "\n",
    "Finally, we define the market status of stock $m_j$ on day $t$, given by $m_j^{(t)} = \\sum_{n_i^{(t)} \\in N^{(t)}} \\alpha_i^{(t)} V_i^{(t)}$. This is the input to the classifier that you will build and train on the dataset to learn the stock embeddings $\\{s_j\\}_{1 \\leq j \\leq 15}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) a) Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6674"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## do it for one stock, AAPL\n",
    "aapl = data[data['Ticker'] == 'AAPL']\n",
    "len(aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set kappa to be max number of articles for a given day\n",
    "kappa = np.max(aapl.groupby('Date').count()['index'])\n",
    "kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove dates that have < 4 articles\n",
    "keep_dates = aapl[\"Date\"].unique()[(aapl.groupby(\"Date\").count()[\"index\"] >= 4)]\n",
    "keep_indices = [aapl.loc[i, \"Date\"] in keep_dates for i in aapl.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now all dates have 4 <= i < 12 articles\n",
    "aapl_processed = aapl[drop_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dates  = sorted(aapl_processed['Date'].unique())\n",
    "num_sequences = len(sorted_dates[4:])\n",
    "num_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-4312f2629db4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_aapl_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkappa\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_append\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpadded_aapl_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7083\u001b[0m             \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7084\u001b[0m             \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7085\u001b[0;31m             \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7086\u001b[0m         )\n\u001b[1;32m   7087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             )\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m             b = make_block(\n\u001b[0;32m-> 2022\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m             )\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36mconcat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# np.concatenate which has them both implemented is compiled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mtyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dtype_kinds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0m_contains_datetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datetime\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0m_contains_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"period\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36mget_dtype_kinds\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_bool_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bool\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_bool_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;31m# now we use the special definition for Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;31m# TODO(jreback)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_typ\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pad processed df to include kappa entries for each date\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "padded_aapl_proc = aapl_processed.copy(deep = True)\n",
    "\n",
    "for date in aapl_processed['Date'].unique(): \n",
    "    df_date = aapl_processed[aapl_processed['Date'] == date]\n",
    "    \n",
    "    if len(df_date) <  kappa: \n",
    "        row_append = df_date.head(1)\n",
    "        #change value and key vec to be 0 for appended rows\n",
    "        row_append.iloc[:, row_append.columns.get_loc('K0') : row_append.columns.get_loc('V299') + 1] = 0\n",
    "        \n",
    "        #temp variable to fix annpying pandas append tendencies\n",
    "        temp = padded_aapl_proc\n",
    "        for i in range(kappa - len(df_date)): \n",
    "            temp = temp.append(row_append, ignore_index = True)\n",
    "            \n",
    "        padded_aapl_proc = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_aapl_proc = padded_aapl_proc.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_aapl_proc)/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have processed our data to include only robust inputs, let's do a quick refresher of what your initial input to the neural network is supposed to look like, and what dimensions it will have. Our key vectors for day $t$ are  $K_i^{(t)} \\in \\mathbb{R}^{64}$, and we have at most $\\kappa$ articles per day. Thus, for any given day $t$, we can treat the input as $\\begin{bmatrix} K_1^{(t)} & \\cdots & K_\\kappa^{(t)} \\end{bmatrix} \\in \\mathbb{R}^{64 \\times \\kappa}$. Since our network uses five market vectors ($m^{(t - 4)}, \\ldots, m^{(t)}$) for predicting stock price movement on any given day, we must pass in a sequence of $\\kappa \\cdot 5$ key vectors. \n",
    "\n",
    "So each input looks like $\\begin{bmatrix} K_1^{(t - 4)} & \\cdots & K_\\kappa^{(t - 4)} & \\cdots \\cdots & K_1^{(t)} & \\cdots & K_\\kappa^{(t)} \\end{bmatrix} \\in \\mathbb{R}^{64 \\times 5\\kappa}$. Then, assuming we have $k$ such datapoints (or in our case, $5$ day sequences) in our training dataset, our input is thus:\n",
    "$\\begin{bmatrix} \n",
    "K_1^{(t_1 - 4)} & \\cdots & K_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & K_1^{(t_1)} & \\cdots & K_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "K_1^{(t_k - 4)} & \\cdots & K_\\kappa^{(t_k - 4)} & \\cdots \\cdots & K_1^{(t_k)} & \\cdots & K_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{64k \\times 5\\kappa}$\n",
    "where each row represents a different sequence of $5$ days for the stock key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = num_sequences\n",
    "X_in = np.zeros((64*k, 5*kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through rows, step size of 64 to account for size of key vectors\n",
    "for i in range(k): \n",
    "    dates = sorted_dates[i : i + 5]\n",
    "    \n",
    "    #counter to keep track of column index\n",
    "    counter = 0\n",
    "    for date in dates: \n",
    "        df = aapl_processed[aapl_processed['Date'] == date]\n",
    "        sub_mat = np.array(df.iloc[:, df.columns.get_loc('K0') : df.columns.get_loc('K63') + 1]).T\n",
    "        X_in[64 * i : 64 * i + 64, counter : counter + sub_mat.shape[1]] = sub_mat\n",
    "        \n",
    "        #increment by kappa to go to next day in sequence\n",
    "        counter += kappa\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50688"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50688, 60)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84720065,  1.99831985,  1.19001225, ...,  1.27729124,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.01052767, -0.82558529, -0.4053802 , ..., -1.03439441,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.99316981, -1.1317625 , -1.35804945, ..., -1.81819254,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.52824199, -0.17233396,  0.72696523, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.52783575, -0.76645522,  0.28038069, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.34940266,  0.26867361,  0.95655539, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) b) Generating classifier labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier aims to predict price movement - in this notebook, we define movement in terms of log returns, the difference in log prices for a given day t, and a preceding day, t-1. Note that due to the nature of the dataset, we do not have a perfectly contiguous sequence of days, however, we use the next best approximation (i.e. t-2, if it is available, in place of t-1). Using these log returns, we binarize price movement: if returns are > 0 on day t, then $ y^t $ = 1, else 0. \n",
    "\n",
    "You can find historical stock price data on http://finance.yahoo.com, and use close prices to calculate log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv('AAPL.csv')\n",
    "prices['log_returns'] = np.log(prices['Close']) - np.log(prices['Close'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date format\n",
    "prices['Date'] = prices['Date'].apply(lambda x: int(x.replace('-', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>log_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20110131</td>\n",
       "      <td>11.992857</td>\n",
       "      <td>12.144286</td>\n",
       "      <td>11.939285</td>\n",
       "      <td>12.118571</td>\n",
       "      <td>10.369199</td>\n",
       "      <td>377246800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20110201</td>\n",
       "      <td>12.189285</td>\n",
       "      <td>12.344643</td>\n",
       "      <td>12.177857</td>\n",
       "      <td>12.322500</td>\n",
       "      <td>10.543692</td>\n",
       "      <td>426633200</td>\n",
       "      <td>0.016688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20110202</td>\n",
       "      <td>12.301785</td>\n",
       "      <td>12.330358</td>\n",
       "      <td>12.269643</td>\n",
       "      <td>12.297143</td>\n",
       "      <td>10.521996</td>\n",
       "      <td>258955200</td>\n",
       "      <td>-0.002060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20110203</td>\n",
       "      <td>12.278571</td>\n",
       "      <td>12.294286</td>\n",
       "      <td>12.091071</td>\n",
       "      <td>12.265715</td>\n",
       "      <td>10.495105</td>\n",
       "      <td>393797600</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20110204</td>\n",
       "      <td>12.272857</td>\n",
       "      <td>12.382143</td>\n",
       "      <td>12.268214</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>10.588613</td>\n",
       "      <td>321840400</td>\n",
       "      <td>0.008870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date       Open       High        Low      Close  Adj Close     Volume  \\\n",
       "0  20110131  11.992857  12.144286  11.939285  12.118571  10.369199  377246800   \n",
       "1  20110201  12.189285  12.344643  12.177857  12.322500  10.543692  426633200   \n",
       "2  20110202  12.301785  12.330358  12.269643  12.297143  10.521996  258955200   \n",
       "3  20110203  12.278571  12.294286  12.091071  12.265715  10.495105  393797600   \n",
       "4  20110204  12.272857  12.382143  12.268214  12.375000  10.588613  321840400   \n",
       "\n",
       "   log_returns  \n",
       "0          NaN  \n",
       "1     0.016688  \n",
       "2    -0.002060  \n",
       "3    -0.002559  \n",
       "4     0.008870  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate labels for each of the k sequences for the AAPL stock\n",
    "labels = []\n",
    "for date in sorted_dates[4:]:\n",
    "    log_ret = prices[prices['Date'] == date]['log_returns'].values\n",
    "    \n",
    "    if (len(log_ret) == 0): \n",
    "        labels.append(np.random.choice([1, 0]))\n",
    "        continue\n",
    "    \n",
    "    if (log_ret[0]) > 0: \n",
    "        labels.append(1)\n",
    "    else: \n",
    "        labels.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Building the pre-classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our main classifier, recall that we need to do some preprocessing to get from our input above to the market vectors that are being used in the classifier. The documentation for tensorflow and Keras will help a lot with some of the manipulations required for this section. In particular, the documentation for `Lambda` layers, `Dense` layers, `Concatenate` layers, `keras.activations.softmax` (which we have imported for you), and `tf.split` may be helpful. The outline of the necessary preprocessing steps is as follows:\n",
    "\n",
    "1. As above, the input layer is of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "K_1^{(t_1 - 4)} & \\cdots & K_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & K_1^{(t_1)} & \\cdots & K_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "K_1^{(t_k - 4)} & \\cdots & K_\\kappa^{(t_k - 4)} & \\cdots \\cdots & K_1^{(t_k)} & \\cdots & K_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{64k \\times 5\\kappa}$\n",
    "<br>\n",
    "\n",
    "2. By treating the stock embedding $s$ as a weight from the input layer to the first hidden layer, generate layer 1 of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "score_1^{(t_1 - 4)} & \\cdots & score_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & score_1^{(t_1)} & \\cdots & score_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "score_1^{(t_k - 4)} & \\cdots & score_\\kappa^{(t_k - 4)} & \\cdots \\cdots & score_1^{(t_k)} & \\cdots & score_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k \\times 5\\kappa}$ <br>\n",
    "*Hint:* Remember, we define $score_i^{(t)} = K_i^{(t)} \\cdot s$. Moreover, look at the documentation for applying Dense layers to rank $> 2$ tensors in Keras and see if you need to modify the input in some way before the input layer.\n",
    "<br>\n",
    "\n",
    "3. Apply softmax activations appropriately to generate layer 2 of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "\\alpha_1^{(t_1 - 4)} & \\cdots & \\alpha_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & \\alpha_1^{(t_1)} & \\cdots & \\alpha_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "\\alpha_1^{(t_k - 4)} & \\cdots & \\alpha_\\kappa^{(t_k - 4)} & \\cdots \\cdots & \\alpha_1^{(t_k)} & \\cdots & \\alpha_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k \\times 5\\kappa}$ <br>\n",
    "*Hint:* Remember, we define $\\alpha_i^{(t)} = \\displaystyle \\frac{\\exp(score_i^{(t)})}{\\sum_{j \\in [\\kappa], j \\neq i}exp(score_j^{(t)})}$.\n",
    "<br>\n",
    "\n",
    "4. Finally, the last layer before the classifier, layer 3, contains the market vectors and must be of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "m^{(t_1 - 4)} & \\cdots & m^{(t_1)}\\\\ \n",
    "\\vdots & & \\vdots \\\\ \n",
    "m^{(t_k - 4)} & \\cdots & m^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{300k \\times 5}$ <br>\n",
    "*Hint:* Remember, we define $m_i^{(t)} = \\displaystyle \\sum_{i \\in [\\kappa]} \\alpha_i^{(t)} V_i^{(t)}$. You may find the custom activation function that we have defined for you helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def market_activation(v):\n",
    "    '''\n",
    "    Input: Matrix v of shape (None, 300k, kappa) whose columns represent value vectors\n",
    "    Returns: Function custom_sum(x)\n",
    "    '''\n",
    "    def custom_sum(x):\n",
    "        '''\n",
    "        Input: Tensor x of shape (None, k, kappa) with alpha values from layer 3\n",
    "        Returns: Tensor m of shape (None, k, 1, 300) that represents the market vectors as defined above\n",
    "        '''\n",
    "        k = x.shape[1]\n",
    "        kappa = x.shape[2]\n",
    "        rows = []\n",
    "        for i in range(k):\n",
    "            alpha_sum = np.zeros((1, 1, 300))\n",
    "            for j in range(kappa):\n",
    "                value_vec = v[300 * i : 300* i + 300, j]\n",
    "                value_vec = tf.reshape(tf.expand_dims(value_vec, axis=2), (1, 1, 300))\n",
    "                print(x[:, i, j])\n",
    "                print(value_vec)\n",
    "                alpha_sum += x[:, i, j] * value_vec\n",
    "            rows.append(alpha_sum)\n",
    "        m = tf.concat(rows, axis=1)\n",
    "        assert m.shape[1] == k and m.shape[2] == 1 and m.shape[3] == 300\n",
    "        return m\n",
    "    \n",
    "    return custom_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Input, Dense, GRU, Bidirectional, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.activations import softmax\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform input by transposing each key vector in the above matrix, i.e. 64k x 5kappa -> k x (64 * 5kappa)\n",
    "rows = []\n",
    "for i in range(0, 64 * k, 64):\n",
    "    cols = [X_in[i : i + 64, j] for j in range(5 * kappa)]\n",
    "    to_row = [c.T for c in cols]\n",
    "    rows.append(np.array(to_row).reshape(1, 64 * 5 * kappa))\n",
    "X_in_mod = np.vstack(rows)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in_mod, labels, test_size=0.2)\n",
    "\n",
    "x = Input(shape = X_train.shape, name=\"Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(x, num_or_size_splits = 5 * kappa, axis=2)\n",
    "#each tensor in cols has shape (None, k, 64)\n",
    "\n",
    "#create (5 * kappa) dense layers, 1 for each tensor in cols, and concatenate them together to get layer 1\n",
    "dense_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    dense_layers.append(Dense(1, use_bias=False)(lambd_layer))\n",
    "\n",
    "layer1 = Concatenate(name=\"Layer1\")(dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(layer1, num_or_size_splits = 5, axis=2)\n",
    "#each tensor in cols has shape (None, k, kappa)\n",
    "\n",
    "#create 5 softmax activations, 1 for each tensor in cols, and concatenate them together to get layer 2\n",
    "softmax_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    softmax_layers.append(softmax(lambd_layer, axis=2))\n",
    "\n",
    "layer2 = Concatenate(name=\"Layer2\")(softmax_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = tf.split(layer1, num_or_size_splits = 5, axis=2)\n",
    "#each tensor in cols has shape (None, k, kappa)\n",
    "\n",
    "#create 5 market activations, 1 for each tensor in cols, and concatenate them together to get layer 3\n",
    "\n",
    "#counter to keep track of start index for dates\n",
    "counter = 0\n",
    "market_layers = []\n",
    "for t in cols:\n",
    "    dates = sorted_dates[counter:792 + counter]\n",
    "    v_inp = np.array([])\n",
    "    \n",
    "    for date in dates: \n",
    "        df = padded_aapl_proc[padded_aapl_proc['Date'] == date]\n",
    "        value_vecs = np.array(df.iloc[:, df.columns.get_loc('V0') : df.columns.get_loc('V299') + 1]).T\n",
    "        \n",
    "        if len(v_inp) == 0: \n",
    "            v_inp = value_vecs\n",
    "    \n",
    "        else: \n",
    "            v_inp = np.vstack((v_inp, value_vecs))\n",
    "        \n",
    "    counter += 1\n",
    "    market_layers.append(market_activation(v_inp)(t))\n",
    "    print(\"col {} completed\".format(t))\n",
    "tf.concat(axis=1)\n",
    "\n",
    "layer3 = Concatenate(name=\"Layer3\", axis=1)(market_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(layer2, num_or_size_splits=5, axis=2)\n",
    "cols = [tf.reduce_mean(c, axis=2, keepdims=True) for c in cols]\n",
    "layer3 = Concatenate(name=\"Layer3\", axis=2)(cols)\n",
    "bigru = Bidirectional(GRU(5, activation = 'relu'))(layer3)\n",
    "pred = Dense(x.shape[1], activation = 'sigmoid')(bigru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 633, 3840)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_46 (TensorFlo [(None, 633, 64), (N 0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_351 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_352 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_353 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_354 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][3]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_355 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][4]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_356 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][5]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_357 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][6]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_358 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][7]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_359 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][8]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_360 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][9]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_361 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][10]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_362 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][11]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_363 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][12]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_364 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][13]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_365 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][14]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_366 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][15]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_367 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][16]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_368 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][17]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_369 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][18]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_370 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][19]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_371 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][20]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_372 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][21]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_373 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][22]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_374 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][23]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_375 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][24]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_376 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][25]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_377 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][26]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_378 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][27]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_379 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][28]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_380 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][29]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_381 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][30]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_382 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][31]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_383 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][32]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_384 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][33]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_385 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][34]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_386 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][35]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_387 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][36]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_388 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][37]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_389 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][38]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_390 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][39]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_391 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][40]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_392 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][41]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_393 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][42]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_394 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][43]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_395 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][44]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_396 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][45]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_397 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][46]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_398 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][47]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_399 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][48]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_400 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][49]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_401 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][50]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_402 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][51]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_403 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][52]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_404 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][53]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_405 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][54]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_406 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][55]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_407 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][56]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_408 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][57]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_409 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][58]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_410 (Lambda)             (None, 633, 64)      0           tf_op_layer_split_46[0][59]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_298 (Dense)               (None, 633, 1)       64          lambda_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_299 (Dense)               (None, 633, 1)       64          lambda_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_300 (Dense)               (None, 633, 1)       64          lambda_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_301 (Dense)               (None, 633, 1)       64          lambda_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_302 (Dense)               (None, 633, 1)       64          lambda_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_303 (Dense)               (None, 633, 1)       64          lambda_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_304 (Dense)               (None, 633, 1)       64          lambda_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_305 (Dense)               (None, 633, 1)       64          lambda_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_306 (Dense)               (None, 633, 1)       64          lambda_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_307 (Dense)               (None, 633, 1)       64          lambda_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_308 (Dense)               (None, 633, 1)       64          lambda_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_309 (Dense)               (None, 633, 1)       64          lambda_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_310 (Dense)               (None, 633, 1)       64          lambda_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_311 (Dense)               (None, 633, 1)       64          lambda_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_312 (Dense)               (None, 633, 1)       64          lambda_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_313 (Dense)               (None, 633, 1)       64          lambda_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_314 (Dense)               (None, 633, 1)       64          lambda_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_315 (Dense)               (None, 633, 1)       64          lambda_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_316 (Dense)               (None, 633, 1)       64          lambda_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_317 (Dense)               (None, 633, 1)       64          lambda_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_318 (Dense)               (None, 633, 1)       64          lambda_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_319 (Dense)               (None, 633, 1)       64          lambda_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_320 (Dense)               (None, 633, 1)       64          lambda_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_321 (Dense)               (None, 633, 1)       64          lambda_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_322 (Dense)               (None, 633, 1)       64          lambda_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_323 (Dense)               (None, 633, 1)       64          lambda_376[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_324 (Dense)               (None, 633, 1)       64          lambda_377[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_325 (Dense)               (None, 633, 1)       64          lambda_378[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_326 (Dense)               (None, 633, 1)       64          lambda_379[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_327 (Dense)               (None, 633, 1)       64          lambda_380[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_328 (Dense)               (None, 633, 1)       64          lambda_381[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_329 (Dense)               (None, 633, 1)       64          lambda_382[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_330 (Dense)               (None, 633, 1)       64          lambda_383[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_331 (Dense)               (None, 633, 1)       64          lambda_384[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_332 (Dense)               (None, 633, 1)       64          lambda_385[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_333 (Dense)               (None, 633, 1)       64          lambda_386[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_334 (Dense)               (None, 633, 1)       64          lambda_387[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_335 (Dense)               (None, 633, 1)       64          lambda_388[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_336 (Dense)               (None, 633, 1)       64          lambda_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_337 (Dense)               (None, 633, 1)       64          lambda_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_338 (Dense)               (None, 633, 1)       64          lambda_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_339 (Dense)               (None, 633, 1)       64          lambda_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_340 (Dense)               (None, 633, 1)       64          lambda_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_341 (Dense)               (None, 633, 1)       64          lambda_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_342 (Dense)               (None, 633, 1)       64          lambda_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_343 (Dense)               (None, 633, 1)       64          lambda_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_344 (Dense)               (None, 633, 1)       64          lambda_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_345 (Dense)               (None, 633, 1)       64          lambda_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_346 (Dense)               (None, 633, 1)       64          lambda_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_347 (Dense)               (None, 633, 1)       64          lambda_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_348 (Dense)               (None, 633, 1)       64          lambda_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_349 (Dense)               (None, 633, 1)       64          lambda_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_350 (Dense)               (None, 633, 1)       64          lambda_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_351 (Dense)               (None, 633, 1)       64          lambda_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_352 (Dense)               (None, 633, 1)       64          lambda_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_353 (Dense)               (None, 633, 1)       64          lambda_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_354 (Dense)               (None, 633, 1)       64          lambda_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_355 (Dense)               (None, 633, 1)       64          lambda_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_356 (Dense)               (None, 633, 1)       64          lambda_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_357 (Dense)               (None, 633, 1)       64          lambda_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Layer1 (Concatenate)            (None, 633, 60)      0           dense_298[0][0]                  \n",
      "                                                                 dense_299[0][0]                  \n",
      "                                                                 dense_300[0][0]                  \n",
      "                                                                 dense_301[0][0]                  \n",
      "                                                                 dense_302[0][0]                  \n",
      "                                                                 dense_303[0][0]                  \n",
      "                                                                 dense_304[0][0]                  \n",
      "                                                                 dense_305[0][0]                  \n",
      "                                                                 dense_306[0][0]                  \n",
      "                                                                 dense_307[0][0]                  \n",
      "                                                                 dense_308[0][0]                  \n",
      "                                                                 dense_309[0][0]                  \n",
      "                                                                 dense_310[0][0]                  \n",
      "                                                                 dense_311[0][0]                  \n",
      "                                                                 dense_312[0][0]                  \n",
      "                                                                 dense_313[0][0]                  \n",
      "                                                                 dense_314[0][0]                  \n",
      "                                                                 dense_315[0][0]                  \n",
      "                                                                 dense_316[0][0]                  \n",
      "                                                                 dense_317[0][0]                  \n",
      "                                                                 dense_318[0][0]                  \n",
      "                                                                 dense_319[0][0]                  \n",
      "                                                                 dense_320[0][0]                  \n",
      "                                                                 dense_321[0][0]                  \n",
      "                                                                 dense_322[0][0]                  \n",
      "                                                                 dense_323[0][0]                  \n",
      "                                                                 dense_324[0][0]                  \n",
      "                                                                 dense_325[0][0]                  \n",
      "                                                                 dense_326[0][0]                  \n",
      "                                                                 dense_327[0][0]                  \n",
      "                                                                 dense_328[0][0]                  \n",
      "                                                                 dense_329[0][0]                  \n",
      "                                                                 dense_330[0][0]                  \n",
      "                                                                 dense_331[0][0]                  \n",
      "                                                                 dense_332[0][0]                  \n",
      "                                                                 dense_333[0][0]                  \n",
      "                                                                 dense_334[0][0]                  \n",
      "                                                                 dense_335[0][0]                  \n",
      "                                                                 dense_336[0][0]                  \n",
      "                                                                 dense_337[0][0]                  \n",
      "                                                                 dense_338[0][0]                  \n",
      "                                                                 dense_339[0][0]                  \n",
      "                                                                 dense_340[0][0]                  \n",
      "                                                                 dense_341[0][0]                  \n",
      "                                                                 dense_342[0][0]                  \n",
      "                                                                 dense_343[0][0]                  \n",
      "                                                                 dense_344[0][0]                  \n",
      "                                                                 dense_345[0][0]                  \n",
      "                                                                 dense_346[0][0]                  \n",
      "                                                                 dense_347[0][0]                  \n",
      "                                                                 dense_348[0][0]                  \n",
      "                                                                 dense_349[0][0]                  \n",
      "                                                                 dense_350[0][0]                  \n",
      "                                                                 dense_351[0][0]                  \n",
      "                                                                 dense_352[0][0]                  \n",
      "                                                                 dense_353[0][0]                  \n",
      "                                                                 dense_354[0][0]                  \n",
      "                                                                 dense_355[0][0]                  \n",
      "                                                                 dense_356[0][0]                  \n",
      "                                                                 dense_357[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_47 (TensorFlo [(None, 633, 12), (N 0           Layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_411 (Lambda)             (None, 633, 12)      0           tf_op_layer_split_47[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_412 (Lambda)             (None, 633, 12)      0           tf_op_layer_split_47[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_413 (Lambda)             (None, 633, 12)      0           tf_op_layer_split_47[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_414 (Lambda)             (None, 633, 12)      0           tf_op_layer_split_47[0][3]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_415 (Lambda)             (None, 633, 12)      0           tf_op_layer_split_47[0][4]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_20 (TensorFlowO [(None, 633, 1)]     0           lambda_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_21 (TensorFlowO [(None, 633, 1)]     0           lambda_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_22 (TensorFlowO [(None, 633, 1)]     0           lambda_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_23 (TensorFlowO [(None, 633, 1)]     0           lambda_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_24 (TensorFlowO [(None, 633, 1)]     0           lambda_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_20 (TensorFlowO [(None, 633, 12)]    0           lambda_411[0][0]                 \n",
      "                                                                 tf_op_layer_Max_20[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_21 (TensorFlowO [(None, 633, 12)]    0           lambda_412[0][0]                 \n",
      "                                                                 tf_op_layer_Max_21[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_22 (TensorFlowO [(None, 633, 12)]    0           lambda_413[0][0]                 \n",
      "                                                                 tf_op_layer_Max_22[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_23 (TensorFlowO [(None, 633, 12)]    0           lambda_414[0][0]                 \n",
      "                                                                 tf_op_layer_Max_23[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_24 (TensorFlowO [(None, 633, 12)]    0           lambda_415[0][0]                 \n",
      "                                                                 tf_op_layer_Max_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_20 (TensorFlowO [(None, 633, 12)]    0           tf_op_layer_Sub_20[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_21 (TensorFlowO [(None, 633, 12)]    0           tf_op_layer_Sub_21[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_22 (TensorFlowO [(None, 633, 12)]    0           tf_op_layer_Sub_22[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_23 (TensorFlowO [(None, 633, 12)]    0           tf_op_layer_Sub_23[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_24 (TensorFlowO [(None, 633, 12)]    0           tf_op_layer_Sub_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4777 (TensorFlo [(None, 633, 1)]     0           tf_op_layer_Exp_20[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4778 (TensorFlo [(None, 633, 1)]     0           tf_op_layer_Exp_21[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4779 (TensorFlo [(None, 633, 1)]     0           tf_op_layer_Exp_22[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4780 (TensorFlo [(None, 633, 1)]     0           tf_op_layer_Exp_23[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4781 (TensorFlo [(None, 633, 1)]     0           tf_op_layer_Exp_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_20 (TensorF [(None, 633, 12)]    0           tf_op_layer_Exp_20[0][0]         \n",
      "                                                                 tf_op_layer_Sum_4777[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_21 (TensorF [(None, 633, 12)]    0           tf_op_layer_Exp_21[0][0]         \n",
      "                                                                 tf_op_layer_Sum_4778[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_22 (TensorF [(None, 633, 12)]    0           tf_op_layer_Exp_22[0][0]         \n",
      "                                                                 tf_op_layer_Sum_4779[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_23 (TensorF [(None, 633, 12)]    0           tf_op_layer_Exp_23[0][0]         \n",
      "                                                                 tf_op_layer_Sum_4780[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_24 (TensorF [(None, 633, 12)]    0           tf_op_layer_Exp_24[0][0]         \n",
      "                                                                 tf_op_layer_Sum_4781[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Layer2 (Concatenate)            (None, 633, 60)      0           tf_op_layer_RealDiv_20[0][0]     \n",
      "                                                                 tf_op_layer_RealDiv_21[0][0]     \n",
      "                                                                 tf_op_layer_RealDiv_22[0][0]     \n",
      "                                                                 tf_op_layer_RealDiv_23[0][0]     \n",
      "                                                                 tf_op_layer_RealDiv_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_49 (TensorFlo [(None, 633, 12), (N 0           Layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_85 (TensorFlow [(None, 633, 1)]     0           tf_op_layer_split_49[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_86 (TensorFlow [(None, 633, 1)]     0           tf_op_layer_split_49[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_87 (TensorFlow [(None, 633, 1)]     0           tf_op_layer_split_49[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_88 (TensorFlow [(None, 633, 1)]     0           tf_op_layer_split_49[0][3]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_89 (TensorFlow [(None, 633, 1)]     0           tf_op_layer_split_49[0][4]       \n",
      "__________________________________________________________________________________________________\n",
      "Layer3 (Concatenate)            (None, 633, 5)       0           tf_op_layer_Mean_85[0][0]        \n",
      "                                                                 tf_op_layer_Mean_86[0][0]        \n",
      "                                                                 tf_op_layer_Mean_87[0][0]        \n",
      "                                                                 tf_op_layer_Mean_88[0][0]        \n",
      "                                                                 tf_op_layer_Mean_89[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 10)           360         Layer3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_359 (Dense)               (None, 633)          6963        bidirectional_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 11,163\n",
      "Trainable params: 11,163\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=x, outputs=pred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 633, 3840) for input Tensor(\"Input_7:0\", shape=(None, 633, 3840), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0), but it was called on an input with incompatible shape (None, 3840).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:719 call\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:888 _run_internal_graph\n        output_tensors = layer(computed_tensors, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2830 call\n        return self._make_op(inputs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2852 _make_op\n        c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension size, given by scalar input 2 must be in range [-2, 2) for '{{node model_12/tf_op_layer_split_46/split_46}} = Split[T=DT_FLOAT, _cloned=true, num_split=60, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model_12/tf_op_layer_split_46/split_46/split_dim, IteratorGetNext)' with input shapes: [], [?,3840] and with computed input tensors: input[0] = <2>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-5b8a5aef8054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2772\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 2774\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[1;32m   2705\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 2706\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:719 call\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:888 _run_internal_graph\n        output_tensors = layer(computed_tensors, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2830 call\n        return self._make_op(inputs)\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2852 _make_op\n        c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\n    /Users/CHIRAG/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension size, given by scalar input 2 must be in range [-2, 2) for '{{node model_12/tf_op_layer_split_46/split_46}} = Split[T=DT_FLOAT, _cloned=true, num_split=60, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model_12/tf_op_layer_split_46/split_46/split_dim, IteratorGetNext)' with input shapes: [], [?,3840] and with computed input tensors: input[0] = <2>.\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633,)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "#transform input by transposing each key vector in the above matrix, i.e. 64k x 5kappa -> k x (64 * 5kappa)\n",
    "rows = []\n",
    "for i in range(0, 64 * k, 64):\n",
    "    cols = [X_in[i : i + 64, j] for j in range(5 * kappa)]\n",
    "    to_row = [c.T for c in cols]\n",
    "    rows.append(np.array(to_row).reshape(1, 64 * 5 * kappa))\n",
    "X_in_mod = np.vstack(rows)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in_mod, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 486144\n  y sizes: 633\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-335-6c7e517250ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# train LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m633\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m633\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 486144\n  y sizes: 633\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# define problem properties\n",
    "n_timesteps = 5\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(n_timesteps, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train LSTM\n",
    "model.fit(X_train.reshape(633 * 64 * 12, 5, 1), np.array(y_train).reshape(633, 1, 1), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.7480255 ],\n",
       "        [ 1.76218472],\n",
       "        [-1.15575253],\n",
       "        [-0.45616578],\n",
       "        [-0.18822806]],\n",
       "\n",
       "       [[-0.74863979],\n",
       "        [ 0.10858612],\n",
       "        [ 0.93778974],\n",
       "        [-0.28595197],\n",
       "        [-1.18828151]],\n",
       "\n",
       "       [[ 0.32384558],\n",
       "        [ 0.79378802],\n",
       "        [ 2.55566559],\n",
       "        [-0.86214029],\n",
       "        [ 1.78243051]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]]])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reshape(633 * 64 * 12, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
