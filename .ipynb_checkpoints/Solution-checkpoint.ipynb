{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## Learning stock embeddings for portfolio optimization using bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Gated Recurrent Units (Bi-GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bi-GRUs for price movement classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, we will focus on training a classifier for 15 stocks from the S&P 500. The goal of our classifier is as follows:\n",
    "We are interested in training a bidirectional RNN model that learns a relationship between news taglines related to the 15 stocks $\\{l_1, \\ldots, l_{15}\\}$ that we have selected and the prices of those stocks. Define $p_i^{(t)}$ to be the price of stock $l_i$ on day $t$. Then, we can formally define our objective as follows:\n",
    "\n",
    "Let $y_i^{(t)} = \\begin{cases} 1 & p_i^{(t)} \\geq p_i^{(t - 1)} \\\\ 0 & p_i^{(t)} < p_i^{(t - 1)} \\end{cases}$. Suppose our dataset $D = \\{N^{(t)}\\}_{t_{in} \\leq t \\leq t_f}$, where $N^{(t)}$ is a collection of all the articles from day $t$ and $t_{in}$ and $t_f$ represent the dates of the earliest and latest articles in our dataset resepctively. Then, we want to learn a mapping $\\hat y_i^{(t)} = f(N^{(t - \\mu)} \\cup \\ldots \\cup N^{(t)})$ such that $\\hat y_i^{(t)}$ accurately predicts $y_i^{(t)}$. More specifically, as is often the case with classification problems, we want to minimize the loss function given by the mean cross-entropy loss for all $15$ stocks:\n",
    "$$\\mathcal{L} = \\frac{1}{15} \\sum_{i = 1}^{15} \\mathcal{L}_i = \\frac{1}{15} \\sum_{i = 1}^{15} \\left( \\frac{-1}{t_f - t_{in}} \\sum_{t = t_{in}}^{t_f} \\big(y_i^{(t)} \\log \\hat y_i^{(t)} + (1 - y_i^{(t)}) \\log (1 - \\hat y_i^{(t)}) \\right)$$\n",
    "Here, we choose to use $\\mu = 4$, so we aim to classify the price movement of stock $l_i$ on day $t$, given by $p_i^{(t)}$, using news information from days $[t-4, t]$, i.e., articles $\\{N^{(t - 4)}, N^{(t - 3)}, N^{(t - 2)}, N^{(t - 1)}, N^{(t)}\\}$. Notice that we are including information from day $t$, so we are not *predicting* the price movement but rather identifying a relationship between the stock price movement and the information contained in the news taglines from day $t$ and the previous 4 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loads word embeddings that we have pre-generated for 15 stocks from the S&P 500. We used news tagline data from Reuters (data sourced from https://github.com/vedic-partap/Event-Driven-Stock-Prediction-using-Deep-Learning/blob/master/input/news_reuters.csv) to create word embeddings for all of the articles in our dataset using a pretrained Spacy encoder and a Word2Vec model that we trained on our data (don't worry if you don't know what this means yet). Our dataset contains news articles from 2011 to 2017 so we should have enough data to build a fairly accurate classifier. You will explore algorithms for generating word embeddings in more detail later in the course but for this assignment, we have done the work for you so that you can focus on building RNN models for your stock movement classifier.\n",
    "\n",
    "For the purposes of our classifier, we are focusing on the 15 stocks from the Reuters dataset for which we have the most data, i.e., news articles.\n",
    "\n",
    "<br>\n",
    "\n",
    "The main idea is to convert all of the qualitative textual information that we have in each article tagline into a quantitative feature that we can use when training our classifier. Let $s_i \\in \\mathbb{R}^{64}$ represent the stock embedding that we are trying to learn for stock $l_i$. We then define the following quantities:\n",
    "\n",
    "Let $n_i^{(t)}$ be a news article from day $t$, for some $1 \\leq i \\leq |N^{(t)}|$. We associate 2 embedding vectors $K_i^{(t)} \\in \\mathbb{R}^{64}$ and $V_i^{(t)} \\in \\mathbb{R}^{300}$ with the article $n_i^{(t)}$, which we have computed for you below. We define $score(n_i^{(t)}, s_j) = K_i^{(t)} \\cdot s_j$ and the softmax variable $$\\alpha_i^{(t)} = \\frac{\\exp(score(n_i^{(t)}, s_j))}{\\sum_{n_k^{(t)} \\in N^{(t)}}exp(score(n_k^{(t)}, s_j))}$$\n",
    "\n",
    "Finally, we define the market status of stock $m_j$ on day $t$, given by $m_j^{(t)} = \\sum_{n_i^{(t)} \\in N^{(t)}} \\alpha_i^{(t)} V_i^{(t)}$. This is the input to the classifier that you will build and train on the dataset to learn the stock embeddings $\\{s_j\\}_{1 \\leq j \\leq 15}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Tagline</th>\n",
       "      <th>Rating</th>\n",
       "      <th>K0</th>\n",
       "      <th>K1</th>\n",
       "      <th>K2</th>\n",
       "      <th>...</th>\n",
       "      <th>V290</th>\n",
       "      <th>V291</th>\n",
       "      <th>V292</th>\n",
       "      <th>V293</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V297</th>\n",
       "      <th>V298</th>\n",
       "      <th>V299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1074</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK Apple Inc has made a \"promising start...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>0.728133</td>\n",
       "      <td>0.074376</td>\n",
       "      <td>-0.844244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184006</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>0.032128</td>\n",
       "      <td>-0.045440</td>\n",
       "      <td>0.027079</td>\n",
       "      <td>-0.100620</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>-0.092093</td>\n",
       "      <td>0.048542</td>\n",
       "      <td>0.109286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1075</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK  April 14 Apple Inc has made a \"promi...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.757790</td>\n",
       "      <td>0.111567</td>\n",
       "      <td>-0.802569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168789</td>\n",
       "      <td>0.039603</td>\n",
       "      <td>0.021292</td>\n",
       "      <td>-0.036883</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>-0.110353</td>\n",
       "      <td>0.025347</td>\n",
       "      <td>-0.084554</td>\n",
       "      <td>0.045670</td>\n",
       "      <td>0.105747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1076</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>COLUMN-How to avoid the trouble coming to the ...</td>\n",
       "      <td>(The opinions expressed here are those of the ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.624152</td>\n",
       "      <td>-0.346050</td>\n",
       "      <td>-1.487509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141506</td>\n",
       "      <td>-0.027039</td>\n",
       "      <td>-0.080825</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>0.018669</td>\n",
       "      <td>-0.056828</td>\n",
       "      <td>-0.052640</td>\n",
       "      <td>-0.169819</td>\n",
       "      <td>-0.033054</td>\n",
       "      <td>0.053817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1077</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>How to avoid the trouble coming to the tech se...</td>\n",
       "      <td>CHICAGO A resounding shot across the bow has b...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.387120</td>\n",
       "      <td>-0.099557</td>\n",
       "      <td>-0.590867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233473</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.113241</td>\n",
       "      <td>-0.027537</td>\n",
       "      <td>-0.119434</td>\n",
       "      <td>-0.074786</td>\n",
       "      <td>-0.072007</td>\n",
       "      <td>-0.049933</td>\n",
       "      <td>0.014863</td>\n",
       "      <td>0.063664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1078</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140415</td>\n",
       "      <td>Apple cannot escape U.S. states' e-book antitr...</td>\n",
       "      <td>NEW YORK Apple Inc on Tuesday lost an attempt ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.824634</td>\n",
       "      <td>-1.637257</td>\n",
       "      <td>-0.352775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232241</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>-0.025965</td>\n",
       "      <td>0.036613</td>\n",
       "      <td>-0.087056</td>\n",
       "      <td>-0.103006</td>\n",
       "      <td>0.076729</td>\n",
       "      <td>-0.153311</td>\n",
       "      <td>0.038894</td>\n",
       "      <td>0.138866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50787</th>\n",
       "      <td>184859</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170209</td>\n",
       "      <td>BRIEF-Ultra Petroleum says Barclays agreed to ...</td>\n",
       "      <td>* Ultra Petroleum- on Feb 8  in connection wit...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.139437</td>\n",
       "      <td>0.682006</td>\n",
       "      <td>0.029171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244216</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>-0.008725</td>\n",
       "      <td>-0.048169</td>\n",
       "      <td>-0.032766</td>\n",
       "      <td>-0.062842</td>\n",
       "      <td>-0.059161</td>\n",
       "      <td>-0.104091</td>\n",
       "      <td>0.010547</td>\n",
       "      <td>0.129130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50788</th>\n",
       "      <td>184860</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170209</td>\n",
       "      <td>MOVES-Barclays  Nasdaq  RenCap  AXA  BC Partners</td>\n",
       "      <td>Feb 9 The following financial services industr...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>1.017802</td>\n",
       "      <td>-0.165982</td>\n",
       "      <td>-0.467275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234947</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>0.064670</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>-0.144732</td>\n",
       "      <td>-0.046366</td>\n",
       "      <td>-0.030195</td>\n",
       "      <td>-0.027131</td>\n",
       "      <td>0.093039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50789</th>\n",
       "      <td>184861</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>Barclays  Citi gave South Africa watchdog info...</td>\n",
       "      <td>JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.044449</td>\n",
       "      <td>-0.042930</td>\n",
       "      <td>0.201579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234416</td>\n",
       "      <td>-0.001098</td>\n",
       "      <td>-0.035648</td>\n",
       "      <td>-0.053637</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>-0.037331</td>\n",
       "      <td>0.048593</td>\n",
       "      <td>-0.019262</td>\n",
       "      <td>-0.030251</td>\n",
       "      <td>0.178724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50790</th>\n",
       "      <td>184862</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>Barclays  Citi helped South Africa with forex ...</td>\n",
       "      <td>JOHANNESBURG Barclays Plc  and Citigroup  appr...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>1.288937</td>\n",
       "      <td>-0.372697</td>\n",
       "      <td>0.197727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247672</td>\n",
       "      <td>0.049712</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>-0.078167</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>0.061589</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>-0.073754</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>0.154577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50791</th>\n",
       "      <td>184863</td>\n",
       "      <td>TAPR</td>\n",
       "      <td>Barclays Inverse US Treasury Composite ETN</td>\n",
       "      <td>20170217</td>\n",
       "      <td>UPDATE 1-Barclays  Citi helped South Africa wi...</td>\n",
       "      <td>JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.336899</td>\n",
       "      <td>-0.299033</td>\n",
       "      <td>0.236128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>-0.072232</td>\n",
       "      <td>0.047836</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>-0.067272</td>\n",
       "      <td>-0.009805</td>\n",
       "      <td>0.183828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50792 rows Ã— 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index Ticker                                        Name      Date  \\\n",
       "0        1074   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "1        1075   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "2        1076   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "3        1077   AAPL                        1-800 FLOWERSCOM Inc  20140414   \n",
       "4        1078   AAPL                        1-800 FLOWERSCOM Inc  20140415   \n",
       "...       ...    ...                                         ...       ...   \n",
       "50787  184859   TAPR  Barclays Inverse US Treasury Composite ETN  20170209   \n",
       "50788  184860   TAPR  Barclays Inverse US Treasury Composite ETN  20170209   \n",
       "50789  184861   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "50790  184862   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "50791  184863   TAPR  Barclays Inverse US Treasury Composite ETN  20170217   \n",
       "\n",
       "                                                Headline  \\\n",
       "0      Apple antitrust compliance off to a promising ...   \n",
       "1      Apple antitrust compliance off to a promising ...   \n",
       "2      COLUMN-How to avoid the trouble coming to the ...   \n",
       "3      How to avoid the trouble coming to the tech se...   \n",
       "4      Apple cannot escape U.S. states' e-book antitr...   \n",
       "...                                                  ...   \n",
       "50787  BRIEF-Ultra Petroleum says Barclays agreed to ...   \n",
       "50788  MOVES-Barclays  Nasdaq  RenCap  AXA  BC Partners    \n",
       "50789  Barclays  Citi gave South Africa watchdog info...   \n",
       "50790  Barclays  Citi helped South Africa with forex ...   \n",
       "50791  UPDATE 1-Barclays  Citi helped South Africa wi...   \n",
       "\n",
       "                                                 Tagline    Rating        K0  \\\n",
       "0      NEW YORK Apple Inc has made a \"promising start...  topStory  0.728133   \n",
       "1      NEW YORK  April 14 Apple Inc has made a \"promi...    normal  0.757790   \n",
       "2      (The opinions expressed here are those of the ...    normal -0.624152   \n",
       "3      CHICAGO A resounding shot across the bow has b...    normal  0.387120   \n",
       "4      NEW YORK Apple Inc on Tuesday lost an attempt ...    normal  0.824634   \n",
       "...                                                  ...       ...       ...   \n",
       "50787  * Ultra Petroleum- on Feb 8  in connection wit...    normal  1.139437   \n",
       "50788  Feb 9 The following financial services industr...  topStory  1.017802   \n",
       "50789  JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...    normal  1.044449   \n",
       "50790  JOHANNESBURG Barclays Plc  and Citigroup  appr...  topStory  1.288937   \n",
       "50791  JOHANNESBURG  Feb 17 Barclays Plc and Citigrou...    normal  1.336899   \n",
       "\n",
       "             K1        K2  ...      V290      V291      V292      V293  \\\n",
       "0      0.074376 -0.844244  ... -0.184006  0.032116  0.032128 -0.045440   \n",
       "1      0.111567 -0.802569  ... -0.168789  0.039603  0.021292 -0.036883   \n",
       "2     -0.346050 -1.487509  ... -0.141506 -0.027039 -0.080825 -0.133556   \n",
       "3     -0.099557 -0.590867  ... -0.233473  0.095700  0.113241 -0.027537   \n",
       "4     -1.637257 -0.352775  ... -0.232241  0.027836 -0.025965  0.036613   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "50787  0.682006  0.029171  ... -0.244216  0.053853 -0.008725 -0.048169   \n",
       "50788 -0.165982 -0.467275  ... -0.234947  0.049924  0.064670  0.022008   \n",
       "50789 -0.042930  0.201579  ... -0.234416 -0.001098 -0.035648 -0.053637   \n",
       "50790 -0.372697  0.197727  ... -0.247672  0.049712  0.028656 -0.078167   \n",
       "50791 -0.299033  0.236128  ... -0.244397  0.040046  0.010388 -0.072232   \n",
       "\n",
       "           V294      V295      V296      V297      V298      V299  \n",
       "0      0.027079 -0.100620  0.032597 -0.092093  0.048542  0.109286  \n",
       "1      0.029685 -0.110353  0.025347 -0.084554  0.045670  0.105747  \n",
       "2      0.018669 -0.056828 -0.052640 -0.169819 -0.033054  0.053817  \n",
       "3     -0.119434 -0.074786 -0.072007 -0.049933  0.014863  0.063664  \n",
       "4     -0.087056 -0.103006  0.076729 -0.153311  0.038894  0.138866  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "50787 -0.032766 -0.062842 -0.059161 -0.104091  0.010547  0.129130  \n",
       "50788  0.025572 -0.144732 -0.046366 -0.030195 -0.027131  0.093039  \n",
       "50789  0.030076 -0.037331  0.048593 -0.019262 -0.030251  0.178724  \n",
       "50790  0.047243  0.061589  0.016127 -0.073754 -0.011532  0.154577  \n",
       "50791  0.047836  0.046529  0.009858 -0.067272 -0.009805  0.183828  \n",
       "\n",
       "[50792 rows x 371 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"embeddings.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each row represents a different news article and is associated with one of the top 15 stocks that we are interested in for our classifier: <br>\n",
    "`['AAPL', 'AMZN', 'BA', 'BCS', 'BP', 'C', 'DB', 'GM', 'GS', 'HSEA', 'HSEB', 'JPM', 'MSFT', 'MS', 'TAPR']`.\n",
    "\n",
    "Additionally, the columns `[K0, ..., K63]` represent the components of the $K_i^{(t)}$ embedding vector and the columns `[V0, ..., V299]` represent the components of the $V_i^{(t)}$ embedding vector for each article $n_i^{(t)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bi-GRU price movement classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) a) Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6674"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## do it for one stock, AAPL\n",
    "aapl = data[data['Ticker'] == 'AAPL']\n",
    "len(aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set kappa to be max number of articles for a given day\n",
    "kappa = np.max(aapl.groupby('Date').count()['index'])\n",
    "kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove dates that have < 4 articles\n",
    "drop_dates = set(aapl['Date'].unique()[(aapl.groupby('Date').count()['index'] < 4)])\n",
    "drop_indices = [not aapl['Date'][i] in drop_dates for i in range(len(aapl))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now all dates have 4 <= i < 12 articles\n",
    "aapl_processed = aapl[drop_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dates  = sorted(aapl_processed['Date'].unique())\n",
    "num_sequences = len(sorted_dates[4:])\n",
    "num_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad processed df to include kappa entries for each date\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "padded_aapl_proc = aapl_processed.copy(deep = True)\n",
    "\n",
    "for date in aapl_processed['Date'].unique(): \n",
    "    df_date = aapl_processed[aapl_processed['Date'] == date]\n",
    "    \n",
    "    if len(df_date) <  kappa: \n",
    "        row_append = df_date.head(1)\n",
    "        #change value and key vec to be 0 for appended rows\n",
    "        row_append.iloc[:, row_append.columns.get_loc('K0') : row_append.columns.get_loc('V299') + 1] = 0\n",
    "        \n",
    "        #temp variable to fix annpying pandas append tendencies\n",
    "        temp = padded_aapl_proc\n",
    "        for i in range(kappa - len(df_date)): \n",
    "            temp = temp.append(row_append, ignore_index = True)\n",
    "            \n",
    "        padded_aapl_proc = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_aapl_proc = padded_aapl_proc.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_aapl_proc)/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have processed our data to include only robust inputs, let's do a quick refresher of what your initial input to the neural network is supposed to look like, and what dimensions it will have. Our key vectors for day $t$ are  $K_i^{(t)} \\in \\mathbb{R}^{64}$, and we have at most $\\kappa$ articles per day. Thus, for any given day $t$, we can treat the input as $\\begin{bmatrix} K_1^{(t)} & \\cdots & K_\\kappa^{(t)} \\end{bmatrix} \\in \\mathbb{R}^{64 \\times \\kappa}$. Since our network uses five market vectors ($m^{(t - 4)}, \\ldots, m^{(t)}$) for predicting stock price movement on any given day, we must pass in a sequence of $\\kappa \\cdot 5$ key vectors. \n",
    "\n",
    "So each input looks like $\\begin{bmatrix} K_1^{(t - 4)} & \\cdots & K_\\kappa^{(t - 4)} & \\cdots \\cdots & K_1^{(t)} & \\cdots & K_\\kappa^{(t)} \\end{bmatrix} \\in \\mathbb{R}^{64 \\times 5\\kappa}$. Then, assuming we have $k$ such datapoints (or in our case, $5$ day sequences) in our training dataset, our input is thus:\n",
    "$\\begin{bmatrix} \n",
    "K_1^{(t_1 - 4)} & \\cdots & K_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & K_1^{(t_1)} & \\cdots & K_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "K_1^{(t_k - 4)} & \\cdots & K_\\kappa^{(t_k - 4)} & \\cdots \\cdots & K_1^{(t_k)} & \\cdots & K_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{64k \\times 5\\kappa}$\n",
    "where each row represents a different sequence of $5$ days for the stock key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = num_sequences\n",
    "X_in = np.zeros((64*k, 5*kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through rows, step size of 64 to account for size of key vectors\n",
    "for i in range(0, 64*num_sequences, 64): \n",
    "    dates = sorted_dates[i : i + 5]\n",
    "    \n",
    "    #counter to keep track of column index\n",
    "    counter = 0\n",
    "    for date in dates: \n",
    "        df = aapl_processed[aapl_processed['Date'] == date]\n",
    "        sub_mat = np.array(df.iloc[:, df.columns.get_loc('K0') : df.columns.get_loc('K63') + 1]).T\n",
    "        X_in[i : i + 64, counter : counter + sub_mat.shape[1]] = sub_mat\n",
    "        \n",
    "        #increment by kappa to go to next day in sequence\n",
    "        counter += kappa\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50688"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50688, 60)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84720065,  1.99831985,  1.19001225, ...,  1.27729124,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.01052767, -0.82558529, -0.4053802 , ..., -1.03439441,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.99316981, -1.1317625 , -1.35804945, ..., -1.81819254,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) b) Generating classifier labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier aims to predict price movement - in this notebook, we define movement in terms of log returns, the difference in log prices for a given day t, and a preceding day, t-1. Note that due to the nature of the dataset, we do not have a perfectly contiguous sequence of days, however, we use the next best approximation (i.e. t-2, if it is available, in place of t-1). Using these log returns, we binarize price movement: if returns are > 0 on day t, then $ y^t $ = 1, else 0. \n",
    "\n",
    "You can find historical stock price data on finance.yahoo.com, and use close prices to calculate log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv('AAPL.csv')\n",
    "prices['log_returns'] = np.log(prices['Close']) - np.log(prices['Close'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date format\n",
    "prices['Date'] = prices['Date'].apply(lambda x: int(x.replace('-', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>log_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20110131</td>\n",
       "      <td>11.992857</td>\n",
       "      <td>12.144286</td>\n",
       "      <td>11.939285</td>\n",
       "      <td>12.118571</td>\n",
       "      <td>10.369199</td>\n",
       "      <td>377246800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20110201</td>\n",
       "      <td>12.189285</td>\n",
       "      <td>12.344643</td>\n",
       "      <td>12.177857</td>\n",
       "      <td>12.322500</td>\n",
       "      <td>10.543692</td>\n",
       "      <td>426633200</td>\n",
       "      <td>0.016688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20110202</td>\n",
       "      <td>12.301785</td>\n",
       "      <td>12.330358</td>\n",
       "      <td>12.269643</td>\n",
       "      <td>12.297143</td>\n",
       "      <td>10.521996</td>\n",
       "      <td>258955200</td>\n",
       "      <td>-0.002060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20110203</td>\n",
       "      <td>12.278571</td>\n",
       "      <td>12.294286</td>\n",
       "      <td>12.091071</td>\n",
       "      <td>12.265715</td>\n",
       "      <td>10.495105</td>\n",
       "      <td>393797600</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20110204</td>\n",
       "      <td>12.272857</td>\n",
       "      <td>12.382143</td>\n",
       "      <td>12.268214</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>10.588613</td>\n",
       "      <td>321840400</td>\n",
       "      <td>0.008870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date       Open       High        Low      Close  Adj Close     Volume  \\\n",
       "0  20110131  11.992857  12.144286  11.939285  12.118571  10.369199  377246800   \n",
       "1  20110201  12.189285  12.344643  12.177857  12.322500  10.543692  426633200   \n",
       "2  20110202  12.301785  12.330358  12.269643  12.297143  10.521996  258955200   \n",
       "3  20110203  12.278571  12.294286  12.091071  12.265715  10.495105  393797600   \n",
       "4  20110204  12.272857  12.382143  12.268214  12.375000  10.588613  321840400   \n",
       "\n",
       "   log_returns  \n",
       "0          NaN  \n",
       "1     0.016688  \n",
       "2    -0.002060  \n",
       "3    -0.002559  \n",
       "4     0.008870  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate labels for each of the k sequences for the AAPL stock\n",
    "labels = []\n",
    "for date in sorted_dates[4:]:\n",
    "    log_ret = prices[prices['Date'] == date]['log_returns'].values\n",
    "    \n",
    "    if (len(log_ret) == 0): \n",
    "        labels.append(np.random.choice([1, 0]))\n",
    "        continue\n",
    "    \n",
    "    if (log_ret[0]) > 0: \n",
    "        labels.append(1)\n",
    "    else: \n",
    "        labels.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Building the pre-classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our main classifier, recall that we need to do some preprocessing to get from our input above to the market vectors that are being used in the classifier. The documentation for tensorflow and Keras will help a lot with some of the manipulations required for this section. In particular, the documentation for `Lambda` layers, `Dense` layers, `Concatenate` layers, `keras.activations.softmax` (which we have imported for you), and `tf.split` may be helpful. The outline of the necessary preprocessing steps is as follows:\n",
    "\n",
    "1. As above, the input layer is of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "K_1^{(t_1 - 4)} & \\cdots & K_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & K_1^{(t_1)} & \\cdots & K_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "K_1^{(t_k - 4)} & \\cdots & K_\\kappa^{(t_k - 4)} & \\cdots \\cdots & K_1^{(t_k)} & \\cdots & K_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{64k \\times 5\\kappa}$\n",
    "<br>\n",
    "\n",
    "2. By treating the stock embedding $s$ as a weight from the input layer to the first hidden layer, generate layer 1 of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "score_1^{(t_1 - 4)} & \\cdots & score_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & score_1^{(t_1)} & \\cdots & score_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "score_1^{(t_k - 4)} & \\cdots & score_\\kappa^{(t_k - 4)} & \\cdots \\cdots & score_1^{(t_k)} & \\cdots & score_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k \\times 5\\kappa}$ <br>\n",
    "*Hint:* Remember, we define $score_i^{(t)} = K_i^{(t)} \\cdot s$. Moreover, look at the documentation for applying Dense layers to rank $> 2$ tensors in Keras and see if you need to modify the input in some way before the input layer.\n",
    "<br>\n",
    "\n",
    "3. Apply softmax activations appropriately to generate layer 2 of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "\\alpha_1^{(t_1 - 4)} & \\cdots & \\alpha_\\kappa^{(t_1 - 4)} & \\cdots \\cdots & \\alpha_1^{(t_1)} & \\cdots & \\alpha_\\kappa^{(t_1)} \\\\\n",
    "\\vdots & & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "\\alpha_1^{(t_k - 4)} & \\cdots & \\alpha_\\kappa^{(t_k - 4)} & \\cdots \\cdots & \\alpha_1^{(t_k)} & \\cdots & \\alpha_\\kappa^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k \\times 5\\kappa}$ <br>\n",
    "*Hint:* Remember, we define $\\alpha_i^{(t)} = \\displaystyle \\frac{\\exp(score_i^{(t)})}{\\sum_{j \\in [\\kappa], j \\neq i}exp(score_j^{(t)})}$.\n",
    "<br>\n",
    "\n",
    "4. Finally, the last layer before the classifier, layer 3, contains the market vectors and must be of the form <br>\n",
    "$\\begin{bmatrix} \n",
    "m^{(t_1 - 4)} & \\cdots & m^{(t_1)}\\\\ \n",
    "\\vdots & & \\vdots \\\\ \n",
    "m^{(t_k)} & \\cdots & m^{(t_k)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{300k \\times 5}$ <br>\n",
    "*Hint:* Remember, we define $m_i^{(t)} = \\displaystyle \\sum_{i \\in [\\kappa]} \\alpha_i^{(t)} V_i^{(t)}$. You may find the custom activation function that we have defined for you helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2] + [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def market_activation(v):\n",
    "    '''\n",
    "    Input: Matrix v of shape (300k, kappa) whose columns represent value vectors\n",
    "    Returns: Function custom_sum(x)\n",
    "    '''\n",
    "    def custom_sum(x):\n",
    "        '''\n",
    "        Input: Tensor x of shape (None, k, kappa) with alpha values from layer 3\n",
    "        Returns: Tensor m of shape (None, 300k, 1) that represents the market vectors as defined above\n",
    "        '''\n",
    "        k = x.shape[1]\n",
    "        kappa = x.shape[2]\n",
    "        cols = []\n",
    "        for i in range(k):\n",
    "            cols.append(sum([x[:, i, j] * v[:, j] for j in range(kappa)]))\n",
    "        m = np.vstack(cols)\n",
    "        return m\n",
    "    \n",
    "    return custom_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Input, Dense, GRU, Bidirectional, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform input by transposing each key vector in the above matrix, i.e. 64k x 5kappa -> k x (64 * 5kappa)\n",
    "rows = []\n",
    "for i in range(0, 64 * k, 64):\n",
    "    cols = [X_in[i : i + 64, j] for j in range(5 * kappa)]\n",
    "    to_row = [c.T for c in cols]\n",
    "    rows.append(np.array(to_row).reshape(1, 64 * 5 * kappa))\n",
    "X_in_mod = np.vstack(rows)\n",
    "\n",
    "x = Input(shape = X_in_mod.shape, name=\"Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(x, num_or_size_splits = 5 * kappa, axis=2)\n",
    "#each tensor in cols has shape (None, k, 64)\n",
    "\n",
    "#create (5 * kappa) dense layers, 1 for each tensor in cols, and concatenate them together to get layer 1\n",
    "dense_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    dense_layers.append(Dense(1, use_bias=False)(lambd_layer))\n",
    "\n",
    "layer1 = Concatenate(name=\"Layer1\")(dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(layer1, num_or_size_splits = 5, axis=2)\n",
    "#each tensor in cols has shape (None, k, kappa)\n",
    "\n",
    "#create 5 softmax activations, 1 for each tensor in cols, and concatenate them together to get layer 2\n",
    "softmax_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    softmax_layers.append(softmax(lambd_layer, axis=2))\n",
    "\n",
    "layer2 = Concatenate(name=\"Layer2\")(softmax_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20110706,\n",
       " 20110707,\n",
       " 20110708,\n",
       " 20110713,\n",
       " 20110714,\n",
       " 20110715,\n",
       " 20110716,\n",
       " 20110717,\n",
       " 20110718,\n",
       " 20110719,\n",
       " 20110720,\n",
       " 20110721,\n",
       " 20110722,\n",
       " 20110725,\n",
       " 20110726,\n",
       " 20110727,\n",
       " 20110728,\n",
       " 20110729,\n",
       " 20110802,\n",
       " 20110803,\n",
       " 20110804,\n",
       " 20110805,\n",
       " 20110807,\n",
       " 20110808,\n",
       " 20110809,\n",
       " 20110810,\n",
       " 20110811,\n",
       " 20110812,\n",
       " 20110817,\n",
       " 20110818,\n",
       " 20110819,\n",
       " 20110820,\n",
       " 20110821,\n",
       " 20110824,\n",
       " 20110825,\n",
       " 20110826,\n",
       " 20110827,\n",
       " 20110828,\n",
       " 20110831,\n",
       " 20110901,\n",
       " 20110903,\n",
       " 20110904,\n",
       " 20110906,\n",
       " 20110907,\n",
       " 20110909,\n",
       " 20110911,\n",
       " 20110912,\n",
       " 20110913,\n",
       " 20110914,\n",
       " 20110915,\n",
       " 20110916,\n",
       " 20110921,\n",
       " 20110922,\n",
       " 20110923,\n",
       " 20110924,\n",
       " 20110925,\n",
       " 20110926,\n",
       " 20110927,\n",
       " 20110928,\n",
       " 20110929,\n",
       " 20110930,\n",
       " 20111002,\n",
       " 20111005,\n",
       " 20111006,\n",
       " 20111007,\n",
       " 20111008,\n",
       " 20111009,\n",
       " 20111011,\n",
       " 20111012,\n",
       " 20111013,\n",
       " 20111014,\n",
       " 20111015,\n",
       " 20111016,\n",
       " 20111018,\n",
       " 20111019,\n",
       " 20111020,\n",
       " 20111024,\n",
       " 20111101,\n",
       " 20111102,\n",
       " 20111103,\n",
       " 20111121,\n",
       " 20111124,\n",
       " 20111125,\n",
       " 20111130,\n",
       " 20111203,\n",
       " 20111205,\n",
       " 20111206,\n",
       " 20111207,\n",
       " 20111208,\n",
       " 20111213,\n",
       " 20111214,\n",
       " 20111215,\n",
       " 20111216,\n",
       " 20111217,\n",
       " 20111221,\n",
       " 20111227,\n",
       " 20111230,\n",
       " 20120104,\n",
       " 20120105,\n",
       " 20120109,\n",
       " 20120110,\n",
       " 20120113,\n",
       " 20120127,\n",
       " 20120130,\n",
       " 20120131,\n",
       " 20120203,\n",
       " 20120207,\n",
       " 20120209,\n",
       " 20120210,\n",
       " 20120212,\n",
       " 20120214,\n",
       " 20120215,\n",
       " 20120216,\n",
       " 20120217,\n",
       " 20120218,\n",
       " 20120221,\n",
       " 20120227,\n",
       " 20120228,\n",
       " 20120229,\n",
       " 20120302,\n",
       " 20120304,\n",
       " 20120305,\n",
       " 20120307,\n",
       " 20120308,\n",
       " 20120310,\n",
       " 20120311,\n",
       " 20120313,\n",
       " 20120314,\n",
       " 20120315,\n",
       " 20120317,\n",
       " 20120321,\n",
       " 20120322,\n",
       " 20120323,\n",
       " 20120324,\n",
       " 20120325,\n",
       " 20120326,\n",
       " 20120327,\n",
       " 20120329,\n",
       " 20120330,\n",
       " 20120403,\n",
       " 20120404,\n",
       " 20120405,\n",
       " 20120406,\n",
       " 20120411,\n",
       " 20120412,\n",
       " 20120413,\n",
       " 20120417,\n",
       " 20120418,\n",
       " 20120419,\n",
       " 20120420,\n",
       " 20120423,\n",
       " 20120427,\n",
       " 20120502,\n",
       " 20120504,\n",
       " 20120507,\n",
       " 20120509,\n",
       " 20120510,\n",
       " 20120515,\n",
       " 20120518,\n",
       " 20120520,\n",
       " 20120521,\n",
       " 20120522,\n",
       " 20120530,\n",
       " 20120531,\n",
       " 20120601,\n",
       " 20120604,\n",
       " 20120605,\n",
       " 20120606,\n",
       " 20120607,\n",
       " 20120611,\n",
       " 20120612,\n",
       " 20120613,\n",
       " 20120614,\n",
       " 20120618,\n",
       " 20120619,\n",
       " 20120620,\n",
       " 20120621,\n",
       " 20120622,\n",
       " 20120627,\n",
       " 20120628,\n",
       " 20120629,\n",
       " 20120630,\n",
       " 20120704,\n",
       " 20120705,\n",
       " 20120706,\n",
       " 20120712,\n",
       " 20120713,\n",
       " 20120715,\n",
       " 20120716,\n",
       " 20120717,\n",
       " 20120718,\n",
       " 20120719,\n",
       " 20120720,\n",
       " 20120722,\n",
       " 20120723,\n",
       " 20120725,\n",
       " 20120726,\n",
       " 20120727,\n",
       " 20120729,\n",
       " 20120731,\n",
       " 20120801,\n",
       " 20120802,\n",
       " 20120804,\n",
       " 20120808,\n",
       " 20120809,\n",
       " 20120810,\n",
       " 20120811,\n",
       " 20120813,\n",
       " 20120814,\n",
       " 20120815,\n",
       " 20120816,\n",
       " 20120817,\n",
       " 20120820,\n",
       " 20120822,\n",
       " 20120823,\n",
       " 20120824,\n",
       " 20120826,\n",
       " 20120829,\n",
       " 20120830,\n",
       " 20120831,\n",
       " 20120901,\n",
       " 20120903,\n",
       " 20120904,\n",
       " 20120906,\n",
       " 20120907,\n",
       " 20120922,\n",
       " 20120923,\n",
       " 20120925,\n",
       " 20120926,\n",
       " 20120927,\n",
       " 20120928,\n",
       " 20120929,\n",
       " 20121004,\n",
       " 20121005,\n",
       " 20121010,\n",
       " 20121012,\n",
       " 20121017,\n",
       " 20121018,\n",
       " 20121019,\n",
       " 20121020,\n",
       " 20121022,\n",
       " 20121023,\n",
       " 20121024,\n",
       " 20121025,\n",
       " 20121026,\n",
       " 20121029,\n",
       " 20121030,\n",
       " 20121031,\n",
       " 20121101,\n",
       " 20121106,\n",
       " 20121107,\n",
       " 20121108,\n",
       " 20121109,\n",
       " 20121111,\n",
       " 20121112,\n",
       " 20121122,\n",
       " 20121127,\n",
       " 20121128,\n",
       " 20121130,\n",
       " 20121201,\n",
       " 20121203,\n",
       " 20121204,\n",
       " 20121206,\n",
       " 20121207,\n",
       " 20121211,\n",
       " 20121213,\n",
       " 20121214,\n",
       " 20121215,\n",
       " 20121216,\n",
       " 20121220,\n",
       " 20121221,\n",
       " 20121224,\n",
       " 20121227,\n",
       " 20121231,\n",
       " 20130102,\n",
       " 20130104,\n",
       " 20130106,\n",
       " 20130108,\n",
       " 20130109,\n",
       " 20130110,\n",
       " 20130115,\n",
       " 20130116,\n",
       " 20130118,\n",
       " 20130123,\n",
       " 20130124,\n",
       " 20130125,\n",
       " 20130126,\n",
       " 20130127,\n",
       " 20130128,\n",
       " 20130130,\n",
       " 20130131,\n",
       " 20130201,\n",
       " 20130204,\n",
       " 20130205,\n",
       " 20130206,\n",
       " 20130207,\n",
       " 20130208,\n",
       " 20130209,\n",
       " 20130210,\n",
       " 20130212,\n",
       " 20130213,\n",
       " 20130214,\n",
       " 20130215,\n",
       " 20130219,\n",
       " 20130220,\n",
       " 20130221,\n",
       " 20130222,\n",
       " 20130223,\n",
       " 20130224,\n",
       " 20130301,\n",
       " 20130302,\n",
       " 20130304,\n",
       " 20130306,\n",
       " 20130307,\n",
       " 20130312,\n",
       " 20130314,\n",
       " 20130315,\n",
       " 20130324,\n",
       " 20130325,\n",
       " 20130327,\n",
       " 20130405,\n",
       " 20130410,\n",
       " 20130411,\n",
       " 20130415,\n",
       " 20130416,\n",
       " 20130417,\n",
       " 20130418,\n",
       " 20130421,\n",
       " 20130422,\n",
       " 20130423,\n",
       " 20130430,\n",
       " 20130502,\n",
       " 20130509,\n",
       " 20130512,\n",
       " 20130513,\n",
       " 20130514,\n",
       " 20130515,\n",
       " 20130516,\n",
       " 20130517,\n",
       " 20130520,\n",
       " 20130521,\n",
       " 20130522,\n",
       " 20130527,\n",
       " 20130529,\n",
       " 20130602,\n",
       " 20130604,\n",
       " 20130606,\n",
       " 20130607,\n",
       " 20130611,\n",
       " 20130618,\n",
       " 20130624,\n",
       " 20130626,\n",
       " 20130703,\n",
       " 20130708,\n",
       " 20130711,\n",
       " 20130715,\n",
       " 20130717,\n",
       " 20130718,\n",
       " 20130723,\n",
       " 20130724,\n",
       " 20130725,\n",
       " 20130727,\n",
       " 20130731,\n",
       " 20130801,\n",
       " 20130803,\n",
       " 20130804,\n",
       " 20130805,\n",
       " 20130807,\n",
       " 20130808,\n",
       " 20130815,\n",
       " 20130816,\n",
       " 20130818,\n",
       " 20130820,\n",
       " 20130822,\n",
       " 20130823,\n",
       " 20130827,\n",
       " 20130903,\n",
       " 20130904,\n",
       " 20130905,\n",
       " 20130909,\n",
       " 20130910,\n",
       " 20130911,\n",
       " 20130912,\n",
       " 20130913,\n",
       " 20130916,\n",
       " 20130917,\n",
       " 20130918,\n",
       " 20130919,\n",
       " 20130923,\n",
       " 20130924,\n",
       " 20130925,\n",
       " 20131001,\n",
       " 20131003,\n",
       " 20131004,\n",
       " 20131009,\n",
       " 20131015,\n",
       " 20131016,\n",
       " 20131017,\n",
       " 20131022,\n",
       " 20131023,\n",
       " 20131029,\n",
       " 20131108,\n",
       " 20131109,\n",
       " 20131111,\n",
       " 20131112,\n",
       " 20131115,\n",
       " 20131118,\n",
       " 20131120,\n",
       " 20131126,\n",
       " 20131127,\n",
       " 20131128,\n",
       " 20131202,\n",
       " 20131205,\n",
       " 20131206,\n",
       " 20131211,\n",
       " 20131216,\n",
       " 20131221,\n",
       " 20131222,\n",
       " 20131224,\n",
       " 20131228,\n",
       " 20140103,\n",
       " 20140108,\n",
       " 20140109,\n",
       " 20140110,\n",
       " 20140115,\n",
       " 20140116,\n",
       " 20140117,\n",
       " 20140120,\n",
       " 20140121,\n",
       " 20140122,\n",
       " 20140123,\n",
       " 20140127,\n",
       " 20140130,\n",
       " 20140204,\n",
       " 20140206,\n",
       " 20140207,\n",
       " 20140211,\n",
       " 20140213,\n",
       " 20140214,\n",
       " 20140220,\n",
       " 20140221,\n",
       " 20140223,\n",
       " 20140224,\n",
       " 20140226,\n",
       " 20140227,\n",
       " 20140305,\n",
       " 20140312,\n",
       " 20140318,\n",
       " 20140323,\n",
       " 20140326,\n",
       " 20140327,\n",
       " 20140401,\n",
       " 20140404,\n",
       " 20140407,\n",
       " 20140408,\n",
       " 20140409,\n",
       " 20140410,\n",
       " 20140411,\n",
       " 20140414,\n",
       " 20140415,\n",
       " 20140417,\n",
       " 20140418,\n",
       " 20140421,\n",
       " 20140423,\n",
       " 20140424,\n",
       " 20140425,\n",
       " 20140428,\n",
       " 20140429,\n",
       " 20140503,\n",
       " 20140505,\n",
       " 20140506,\n",
       " 20140508,\n",
       " 20140509,\n",
       " 20140512,\n",
       " 20140514,\n",
       " 20140517,\n",
       " 20140521,\n",
       " 20140527,\n",
       " 20140528,\n",
       " 20140529,\n",
       " 20140602,\n",
       " 20140609,\n",
       " 20140610,\n",
       " 20140611,\n",
       " 20140617,\n",
       " 20140619,\n",
       " 20140620,\n",
       " 20140623,\n",
       " 20140624,\n",
       " 20140625,\n",
       " 20140702,\n",
       " 20140709,\n",
       " 20140710,\n",
       " 20140716,\n",
       " 20140717,\n",
       " 20140721,\n",
       " 20140722,\n",
       " 20140723,\n",
       " 20140724,\n",
       " 20140725,\n",
       " 20140729,\n",
       " 20140805,\n",
       " 20140806,\n",
       " 20140812,\n",
       " 20140815,\n",
       " 20140819,\n",
       " 20140826,\n",
       " 20140827,\n",
       " 20140903,\n",
       " 20140904,\n",
       " 20140905,\n",
       " 20140908,\n",
       " 20140909,\n",
       " 20140910,\n",
       " 20140911,\n",
       " 20140912,\n",
       " 20140915,\n",
       " 20140916,\n",
       " 20140917,\n",
       " 20140918,\n",
       " 20140919,\n",
       " 20140922,\n",
       " 20140924,\n",
       " 20140925,\n",
       " 20140926,\n",
       " 20140929,\n",
       " 20140930,\n",
       " 20141001,\n",
       " 20141008,\n",
       " 20141009,\n",
       " 20141010,\n",
       " 20141014,\n",
       " 20141015,\n",
       " 20141016,\n",
       " 20141020,\n",
       " 20141021,\n",
       " 20141022,\n",
       " 20141023,\n",
       " 20141024,\n",
       " 20141027,\n",
       " 20141028,\n",
       " 20141029,\n",
       " 20141030,\n",
       " 20141031,\n",
       " 20141103,\n",
       " 20141104,\n",
       " 20141106,\n",
       " 20141112,\n",
       " 20141117,\n",
       " 20141119,\n",
       " 20141121,\n",
       " 20141125,\n",
       " 20141201,\n",
       " 20141202,\n",
       " 20141204,\n",
       " 20141211,\n",
       " 20141212,\n",
       " 20141215,\n",
       " 20141216,\n",
       " 20141217,\n",
       " 20141223,\n",
       " 20141228,\n",
       " 20141229,\n",
       " 20150114,\n",
       " 20150115,\n",
       " 20150127,\n",
       " 20150128,\n",
       " 20150129,\n",
       " 20150205,\n",
       " 20150210,\n",
       " 20150211,\n",
       " 20150212,\n",
       " 20150213,\n",
       " 20150214,\n",
       " 20150217,\n",
       " 20150218,\n",
       " 20150219,\n",
       " 20150220,\n",
       " 20150223,\n",
       " 20150224,\n",
       " 20150225,\n",
       " 20150226,\n",
       " 20150227,\n",
       " 20150301,\n",
       " 20150302,\n",
       " 20150303,\n",
       " 20150304,\n",
       " 20150305,\n",
       " 20150306,\n",
       " 20150309,\n",
       " 20150310,\n",
       " 20150311,\n",
       " 20150313,\n",
       " 20150317,\n",
       " 20150318,\n",
       " 20150326,\n",
       " 20150402,\n",
       " 20150408,\n",
       " 20150409,\n",
       " 20150410,\n",
       " 20150413,\n",
       " 20150415,\n",
       " 20150416,\n",
       " 20150422,\n",
       " 20150423,\n",
       " 20150424,\n",
       " 20150427,\n",
       " 20150428,\n",
       " 20150429,\n",
       " 20150430,\n",
       " 20150506,\n",
       " 20150508,\n",
       " 20150511,\n",
       " 20150513,\n",
       " 20150515,\n",
       " 20150518,\n",
       " 20150519,\n",
       " 20150520,\n",
       " 20150521,\n",
       " 20150526,\n",
       " 20150528,\n",
       " 20150529,\n",
       " 20150531,\n",
       " 20150603,\n",
       " 20150604,\n",
       " 20150606,\n",
       " 20150608,\n",
       " 20150609,\n",
       " 20150610,\n",
       " 20150611,\n",
       " 20150617,\n",
       " 20150618,\n",
       " 20150625,\n",
       " 20150630,\n",
       " 20150708,\n",
       " 20150716,\n",
       " 20150720,\n",
       " 20150721,\n",
       " 20150722,\n",
       " 20150723,\n",
       " 20150724,\n",
       " 20150730,\n",
       " 20150803,\n",
       " 20150804,\n",
       " 20150806,\n",
       " 20150812,\n",
       " 20150813,\n",
       " 20150814,\n",
       " 20150820,\n",
       " 20150821,\n",
       " 20150824,\n",
       " 20150825,\n",
       " 20150827,\n",
       " 20150901,\n",
       " 20150903,\n",
       " 20150909,\n",
       " 20150910,\n",
       " 20150914,\n",
       " 20150916,\n",
       " 20150917,\n",
       " 20150918,\n",
       " 20150921,\n",
       " 20150922,\n",
       " 20150923,\n",
       " 20150924,\n",
       " 20150925,\n",
       " 20150928,\n",
       " 20150930,\n",
       " 20151001,\n",
       " 20151002,\n",
       " 20151006,\n",
       " 20151009,\n",
       " 20151013,\n",
       " 20151014,\n",
       " 20151016,\n",
       " 20151019,\n",
       " 20151020,\n",
       " 20151021,\n",
       " 20151026,\n",
       " 20151027,\n",
       " 20151028,\n",
       " 20151029,\n",
       " 20151109,\n",
       " 20151110,\n",
       " 20151111,\n",
       " 20151118,\n",
       " 20151120,\n",
       " 20151124,\n",
       " 20151208,\n",
       " 20151214,\n",
       " 20151215,\n",
       " 20151216,\n",
       " 20151217,\n",
       " 20151218,\n",
       " 20151221,\n",
       " 20151223,\n",
       " 20151230,\n",
       " 20160105,\n",
       " 20160106,\n",
       " 20160107,\n",
       " 20160108,\n",
       " 20160120,\n",
       " 20160122,\n",
       " 20160126,\n",
       " 20160127,\n",
       " 20160128,\n",
       " 20160129,\n",
       " 20160201,\n",
       " 20160202,\n",
       " 20160204,\n",
       " 20160216,\n",
       " 20160217,\n",
       " 20160219,\n",
       " 20160220,\n",
       " 20160222,\n",
       " 20160223,\n",
       " 20160224,\n",
       " 20160225,\n",
       " 20160226,\n",
       " 20160227,\n",
       " 20160229,\n",
       " 20160301,\n",
       " 20160303,\n",
       " 20160304,\n",
       " 20160307,\n",
       " 20160308,\n",
       " 20160310,\n",
       " 20160311,\n",
       " 20160315,\n",
       " 20160316,\n",
       " 20160317,\n",
       " 20160321,\n",
       " 20160322,\n",
       " 20160323,\n",
       " 20160324,\n",
       " 20160329,\n",
       " 20160330,\n",
       " 20160408,\n",
       " 20160414,\n",
       " 20160415,\n",
       " 20160419,\n",
       " 20160420,\n",
       " 20160426,\n",
       " 20160427,\n",
       " 20160428,\n",
       " 20160429,\n",
       " 20160505,\n",
       " 20160512,\n",
       " 20160513,\n",
       " 20160516,\n",
       " 20160517,\n",
       " 20160518,\n",
       " 20160519,\n",
       " 20160524,\n",
       " 20160525,\n",
       " 20160601,\n",
       " 20160603,\n",
       " 20160610,\n",
       " 20160613,\n",
       " 20160617,\n",
       " 20160620,\n",
       " 20160621,\n",
       " 20160726,\n",
       " 20160727,\n",
       " 20160801,\n",
       " 20160802,\n",
       " 20160804,\n",
       " 20160815,\n",
       " 20160816,\n",
       " 20160817,\n",
       " 20160823,\n",
       " 20160830,\n",
       " 20160831,\n",
       " 20160901,\n",
       " 20160907,\n",
       " 20160908,\n",
       " 20160914,\n",
       " 20160915,\n",
       " 20160916,\n",
       " 20160921,\n",
       " 20160926,\n",
       " 20161005,\n",
       " 20161007,\n",
       " 20161019,\n",
       " 20161025,\n",
       " 20161026,\n",
       " 20161027,\n",
       " 20161114,\n",
       " 20161115,\n",
       " 20161116,\n",
       " 20161129,\n",
       " 20161206,\n",
       " 20161207,\n",
       " 20170105,\n",
       " 20170106,\n",
       " 20170112]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_dates[:792])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tf.split(layer1, num_or_size_splits = 5, axis=2)\n",
    "#each tensor in cols has shape (None, k, kappa)\n",
    "\n",
    "#create 5 market activations, 1 for each tensor in cols, and concatenate them together to get layer 3\n",
    "\n",
    "#counter to keep track of start index for dates\n",
    "counter = 0\n",
    "market_layers = []\n",
    "for t in cols:\n",
    "    lambd_layer = Lambda(lambda x:x)(t)\n",
    "    \n",
    "    dates = sorted_dates[counter:792 + counter]\n",
    "    \n",
    "    v_inp = np.array([])\n",
    "    \n",
    "    for date in dates: \n",
    "        df = padded_aapl_proc[padded_aapl_proc['Date'] == date]\n",
    "        value_vecs = np.array(padded_aapl_proc.iloc[:, df.columns.get_loc('V0') : df.columns.get_loc('V299') + 1]).T\n",
    "        \n",
    "        if len(v_inp) == 0: \n",
    "            v_inp = value_vecs\n",
    "    \n",
    "        else: \n",
    "            v_inp = np.vstack((v_inp, value_vecs))\n",
    "            \n",
    "    counter += 1\n",
    "    market_layers.append(market_activation(v_inp)(t))\n",
    "\n",
    "layer3 = Concatenate(name=\"Layer3\")(market_layers)\n",
    "\n",
    "'''\n",
    "for i in range(num_sequences*5): \n",
    "    df = aapl_processed.loc[i:i+kappa, :]\n",
    "    value_vecs = np.array(df.iloc[:, df.columns.get_loc('V0') : df.columns.get_loc('V299') + 1]).T\n",
    "    value_vecs = value_vecs.reshape(value_vecs.shape[0] * value_vecs.shape[1], 1)\n",
    "    print(value_vecs.shape)\n",
    "    \n",
    "    alphas_arr = alphas[i]\n",
    "    \n",
    "    inp = tf.concat(alphas_arr, value_vecs) \n",
    "    print(inp.shape)\n",
    "    #need to check how to pass multiple inputs into Dense layer\n",
    "    linear_sums.append(Activation(custom_sum)(inp))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 792, 3840)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_4 (TensorFlow [(None, 792, 64), (N 0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_240 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_241 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_242 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_243 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_244 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_245 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][5]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_246 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][6]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_247 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][7]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_248 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][8]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_249 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][9]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_250 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][10]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_251 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][11]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_252 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][12]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_253 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][13]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_254 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][14]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_255 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][15]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_256 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][16]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_257 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][17]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_258 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][18]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_259 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][19]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_260 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][20]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_261 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][21]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_262 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][22]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_263 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][23]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_264 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][24]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_265 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][25]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_266 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][26]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_267 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][27]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_268 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][28]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_269 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][29]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_270 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][30]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_271 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][31]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_272 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][32]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_273 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][33]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_274 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][34]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_275 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][35]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_276 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][36]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_277 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][37]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_278 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][38]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_279 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][39]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_280 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][40]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_281 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][41]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_282 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][42]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_283 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][43]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_284 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][44]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_285 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][45]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_286 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][46]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_287 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][47]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_288 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][48]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_289 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][49]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_290 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][50]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_291 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][51]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_292 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][52]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_293 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][53]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_294 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][54]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_295 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][55]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_296 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][56]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_297 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][57]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_298 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][58]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_299 (Lambda)             (None, 792, 64)      0           tf_op_layer_split_4[0][59]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_240 (Dense)               (None, 792, 1)       64          lambda_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_241 (Dense)               (None, 792, 1)       64          lambda_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_242 (Dense)               (None, 792, 1)       64          lambda_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_243 (Dense)               (None, 792, 1)       64          lambda_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_244 (Dense)               (None, 792, 1)       64          lambda_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_245 (Dense)               (None, 792, 1)       64          lambda_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_246 (Dense)               (None, 792, 1)       64          lambda_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_247 (Dense)               (None, 792, 1)       64          lambda_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_248 (Dense)               (None, 792, 1)       64          lambda_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_249 (Dense)               (None, 792, 1)       64          lambda_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_250 (Dense)               (None, 792, 1)       64          lambda_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_251 (Dense)               (None, 792, 1)       64          lambda_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_252 (Dense)               (None, 792, 1)       64          lambda_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_253 (Dense)               (None, 792, 1)       64          lambda_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_254 (Dense)               (None, 792, 1)       64          lambda_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_255 (Dense)               (None, 792, 1)       64          lambda_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_256 (Dense)               (None, 792, 1)       64          lambda_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_257 (Dense)               (None, 792, 1)       64          lambda_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_258 (Dense)               (None, 792, 1)       64          lambda_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_259 (Dense)               (None, 792, 1)       64          lambda_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_260 (Dense)               (None, 792, 1)       64          lambda_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_261 (Dense)               (None, 792, 1)       64          lambda_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_262 (Dense)               (None, 792, 1)       64          lambda_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_263 (Dense)               (None, 792, 1)       64          lambda_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_264 (Dense)               (None, 792, 1)       64          lambda_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_265 (Dense)               (None, 792, 1)       64          lambda_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_266 (Dense)               (None, 792, 1)       64          lambda_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_267 (Dense)               (None, 792, 1)       64          lambda_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_268 (Dense)               (None, 792, 1)       64          lambda_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_269 (Dense)               (None, 792, 1)       64          lambda_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_270 (Dense)               (None, 792, 1)       64          lambda_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_271 (Dense)               (None, 792, 1)       64          lambda_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_272 (Dense)               (None, 792, 1)       64          lambda_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_273 (Dense)               (None, 792, 1)       64          lambda_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_274 (Dense)               (None, 792, 1)       64          lambda_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_275 (Dense)               (None, 792, 1)       64          lambda_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_276 (Dense)               (None, 792, 1)       64          lambda_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_277 (Dense)               (None, 792, 1)       64          lambda_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_278 (Dense)               (None, 792, 1)       64          lambda_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_279 (Dense)               (None, 792, 1)       64          lambda_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_280 (Dense)               (None, 792, 1)       64          lambda_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_281 (Dense)               (None, 792, 1)       64          lambda_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_282 (Dense)               (None, 792, 1)       64          lambda_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_283 (Dense)               (None, 792, 1)       64          lambda_283[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_284 (Dense)               (None, 792, 1)       64          lambda_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_285 (Dense)               (None, 792, 1)       64          lambda_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_286 (Dense)               (None, 792, 1)       64          lambda_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_287 (Dense)               (None, 792, 1)       64          lambda_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_288 (Dense)               (None, 792, 1)       64          lambda_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_289 (Dense)               (None, 792, 1)       64          lambda_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_290 (Dense)               (None, 792, 1)       64          lambda_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_291 (Dense)               (None, 792, 1)       64          lambda_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_292 (Dense)               (None, 792, 1)       64          lambda_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_293 (Dense)               (None, 792, 1)       64          lambda_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_294 (Dense)               (None, 792, 1)       64          lambda_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_295 (Dense)               (None, 792, 1)       64          lambda_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_296 (Dense)               (None, 792, 1)       64          lambda_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_297 (Dense)               (None, 792, 1)       64          lambda_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_298 (Dense)               (None, 792, 1)       64          lambda_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_299 (Dense)               (None, 792, 1)       64          lambda_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Layer1 (Concatenate)            (None, 792, 60)      0           dense_240[0][0]                  \n",
      "                                                                 dense_241[0][0]                  \n",
      "                                                                 dense_242[0][0]                  \n",
      "                                                                 dense_243[0][0]                  \n",
      "                                                                 dense_244[0][0]                  \n",
      "                                                                 dense_245[0][0]                  \n",
      "                                                                 dense_246[0][0]                  \n",
      "                                                                 dense_247[0][0]                  \n",
      "                                                                 dense_248[0][0]                  \n",
      "                                                                 dense_249[0][0]                  \n",
      "                                                                 dense_250[0][0]                  \n",
      "                                                                 dense_251[0][0]                  \n",
      "                                                                 dense_252[0][0]                  \n",
      "                                                                 dense_253[0][0]                  \n",
      "                                                                 dense_254[0][0]                  \n",
      "                                                                 dense_255[0][0]                  \n",
      "                                                                 dense_256[0][0]                  \n",
      "                                                                 dense_257[0][0]                  \n",
      "                                                                 dense_258[0][0]                  \n",
      "                                                                 dense_259[0][0]                  \n",
      "                                                                 dense_260[0][0]                  \n",
      "                                                                 dense_261[0][0]                  \n",
      "                                                                 dense_262[0][0]                  \n",
      "                                                                 dense_263[0][0]                  \n",
      "                                                                 dense_264[0][0]                  \n",
      "                                                                 dense_265[0][0]                  \n",
      "                                                                 dense_266[0][0]                  \n",
      "                                                                 dense_267[0][0]                  \n",
      "                                                                 dense_268[0][0]                  \n",
      "                                                                 dense_269[0][0]                  \n",
      "                                                                 dense_270[0][0]                  \n",
      "                                                                 dense_271[0][0]                  \n",
      "                                                                 dense_272[0][0]                  \n",
      "                                                                 dense_273[0][0]                  \n",
      "                                                                 dense_274[0][0]                  \n",
      "                                                                 dense_275[0][0]                  \n",
      "                                                                 dense_276[0][0]                  \n",
      "                                                                 dense_277[0][0]                  \n",
      "                                                                 dense_278[0][0]                  \n",
      "                                                                 dense_279[0][0]                  \n",
      "                                                                 dense_280[0][0]                  \n",
      "                                                                 dense_281[0][0]                  \n",
      "                                                                 dense_282[0][0]                  \n",
      "                                                                 dense_283[0][0]                  \n",
      "                                                                 dense_284[0][0]                  \n",
      "                                                                 dense_285[0][0]                  \n",
      "                                                                 dense_286[0][0]                  \n",
      "                                                                 dense_287[0][0]                  \n",
      "                                                                 dense_288[0][0]                  \n",
      "                                                                 dense_289[0][0]                  \n",
      "                                                                 dense_290[0][0]                  \n",
      "                                                                 dense_291[0][0]                  \n",
      "                                                                 dense_292[0][0]                  \n",
      "                                                                 dense_293[0][0]                  \n",
      "                                                                 dense_294[0][0]                  \n",
      "                                                                 dense_295[0][0]                  \n",
      "                                                                 dense_296[0][0]                  \n",
      "                                                                 dense_297[0][0]                  \n",
      "                                                                 dense_298[0][0]                  \n",
      "                                                                 dense_299[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_9 (TensorFlow [(None, 792, 12), (N 0           Layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_304 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_305 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_306 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_307 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_308 (Lambda)             (None, 792, 12)      0           tf_op_layer_split_9[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max (TensorFlowOpLa [(None, 792, 1)]     0           lambda_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_1 (TensorFlowOp [(None, 792, 1)]     0           lambda_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_2 (TensorFlowOp [(None, 792, 1)]     0           lambda_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_3 (TensorFlowOp [(None, 792, 1)]     0           lambda_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_4 (TensorFlowOp [(None, 792, 1)]     0           lambda_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 792, 12)]    0           lambda_304[0][0]                 \n",
      "                                                                 tf_op_layer_Max[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(None, 792, 12)]    0           lambda_305[0][0]                 \n",
      "                                                                 tf_op_layer_Max_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_2 (TensorFlowOp [(None, 792, 12)]    0           lambda_306[0][0]                 \n",
      "                                                                 tf_op_layer_Max_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_3 (TensorFlowOp [(None, 792, 12)]    0           lambda_307[0][0]                 \n",
      "                                                                 tf_op_layer_Max_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_4 (TensorFlowOp [(None, 792, 12)]    0           lambda_308[0][0]                 \n",
      "                                                                 tf_op_layer_Max_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp (TensorFlowOpLa [(None, 792, 12)]    0           tf_op_layer_Sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_1 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_2 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_3 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_4 (TensorFlowOp [(None, 792, 12)]    0           tf_op_layer_Sub_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 792, 1)]     0           tf_op_layer_Exp[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_3 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4 (TensorFlowOp [(None, 792, 1)]     0           tf_op_layer_Exp_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorFlow [(None, 792, 12)]    0           tf_op_layer_Exp[0][0]            \n",
      "                                                                 tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_1 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_1[0][0]          \n",
      "                                                                 tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_2 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_2[0][0]          \n",
      "                                                                 tf_op_layer_Sum_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_3 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_3[0][0]          \n",
      "                                                                 tf_op_layer_Sum_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_4 (TensorFl [(None, 792, 12)]    0           tf_op_layer_Exp_4[0][0]          \n",
      "                                                                 tf_op_layer_Sum_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Layer2 (Concatenate)            (None, 792, 60)      0           tf_op_layer_RealDiv[0][0]        \n",
      "                                                                 tf_op_layer_RealDiv_1[0][0]      \n",
      "                                                                 tf_op_layer_RealDiv_2[0][0]      \n",
      "                                                                 tf_op_layer_RealDiv_3[0][0]      \n",
      "                                                                 tf_op_layer_RealDiv_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,840\n",
      "Trainable params: 3,840\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = x, outputs = layer2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_alphas = Concatenate([linear_sums[i] for i in range(len(linear_sums))])\n",
    "bigru = Bidirectional(GRU(num_sequences, activation = 'relu'))(flattened_alphas)\n",
    "pred = Dense(num_sequences, activation = 'sigmoid')(bigru)\n",
    "\n",
    "model = Model(inputs = x, outputs = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
